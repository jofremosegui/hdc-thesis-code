{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/__init__.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/embeddings.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/models.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from torchhd.embeddings import Random, Level\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Encoder ===\n",
    "class HdcGenericEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, out_dimension, ngrams=7, dtype=torch.float32, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.ngrams = ngrams\n",
    "        self.device = device\n",
    "\n",
    "        self.keys = Random(input_size, out_dimension, dtype=dtype, device=device, vsa=\"MAP\")\n",
    "        self.motion_embed = Level(3000, out_dimension, dtype=dtype, low=-3.0, high=3.0, device=device, vsa=\"MAP\")\n",
    "        self.hr_embed = Level(200, out_dimension, dtype=dtype, low=50, high=200, device=device, vsa=\"MAP\")\n",
    "\n",
    "    def batch_generic(self, id, levels, ngram):\n",
    "        batch_size = levels.shape[0]\n",
    "        multiset_list = []\n",
    "        for b in range(batch_size):\n",
    "            level = levels[b]\n",
    "            b_levels = [\n",
    "                torchhd.ngrams(level[0][i : i + ngram], ngram)\n",
    "                for i in range(1, id.shape[0] - ngram + 1)\n",
    "            ]\n",
    "            if len(b_levels) > 0:\n",
    "                b_levels = torch.stack(b_levels)\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id[:-ngram], b_levels)).unsqueeze(0))\n",
    "            else:\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id, level)))\n",
    "        return torch.stack(multiset_list)\n",
    "\n",
    "    def forward(self, channels):\n",
    "        motion = channels[:, :, : self.input_size - 1]\n",
    "        hr = channels[:, :, self.input_size - 1].unsqueeze(-1)\n",
    "\n",
    "        enc_motion = self.motion_embed(motion)\n",
    "        enc_hr = self.hr_embed(hr)\n",
    "        enc = torch.cat([enc_motion, enc_hr], dim=2)\n",
    "        hvs = self.batch_generic(self.keys.weight, enc, self.ngrams)\n",
    "        return torchhd.hard_quantize(torchhd.multiset(hvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder and classifier successfully\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "ENCODER_PATH = \"results/HDC_MLP/20250517_010909/encoder.pt\"\n",
    "CLASSIFIER_PATH = \"results/HDC_MLP/20250517_010909/MLP_model.pkl\"\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "device = torch.device(\"cpu\")\n",
    "# === Load Encoder and Classifier ===\n",
    "encoder = HdcGenericEncoder(input_size=7, out_dimension=5000, ngrams=7, device=device)\n",
    "encoder.load_state_dict(torch.load(ENCODER_PATH, map_location=device))\n",
    "encoder.eval()\n",
    "\n",
    "classifier = joblib.load(CLASSIFIER_PATH)\n",
    "print(\"Loaded encoder and classifier successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "# === Dummy Sample ===\n",
    "def generate_input():\n",
    "    return torch.tensor(np.random.randn(*INPUT_SHAPE).astype(np.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Time (avg ± std): 70.87 ± 40.60 ms\n",
      "Classifier Time (avg ± std): 7.68 ± 5.37 ms\n",
      "Total Inference Time (avg): 78.55 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "    hv = encoder(sample_input)\n",
    "    _ = classifier.predict(hv.numpy())\n",
    "\n",
    "encode_times = []\n",
    "classify_times = []\n",
    "\n",
    "for _ in range(100):\n",
    "    # Generate a new random input each time (or use same one if needed)\n",
    "    input_tensor = torch.randn(1, 800, 7).to(\"cpu\")\n",
    "\n",
    "    # --- Encode ---\n",
    "    start_enc = time.perf_counter()\n",
    "    hv = encoder(input_tensor)\n",
    "    end_enc = time.perf_counter()\n",
    "    encode_times.append((end_enc - start_enc) * 1000)\n",
    "\n",
    "    # --- Classify ---\n",
    "    start_clf = time.perf_counter()\n",
    "    _ = classifier.predict(hv.numpy())\n",
    "    end_clf = time.perf_counter()\n",
    "    classify_times.append((end_clf - start_clf) * 1000)\n",
    "\n",
    "# --- Results ---\n",
    "print(f\"Encoding Time (avg ± std): {np.mean(encode_times):.2f} ± {np.std(encode_times):.2f} ms\")\n",
    "print(f\"Classifier Time (avg ± std): {np.mean(classify_times):.2f} ± {np.std(classify_times):.2f} ms\")\n",
    "print(f\"Total Inference Time (avg): {np.mean(encode_times) + np.mean(classify_times):.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Size: 19.796714782714844\n",
      "Encoder Size: 61.17036819458008\n",
      "Model Size: 80.97 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "classifier_size = os.path.getsize(CLASSIFIER_PATH) / (1024 ** 2)\n",
    "encoder_size = os.path.getsize(ENCODER_PATH) / (1024 ** 2)\n",
    "print(\"Classifier Size:\", classifier_size)\n",
    "print(\"Encoder Size:\", encoder_size)\n",
    "print(f\"Model Size: {(classifier_size + encoder_size):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.09 MB\n",
      "Total Memory Usage After Inference: 963.67 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "# --- Memory Usage ---\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "_ = classifier.predict(encoder(generate_input()).numpy())\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem = mem_after / (1024 ** 2)\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.943 J\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Energy Estimate ---\n",
    "ENERGY_ESTIMATED_WATT = 12\n",
    "avg_time = np.mean(encode_times) + np.mean(classify_times)\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 0.03 MB\n",
      "Total Memory Usage After Inference: 900.41 MB\n",
      "\n",
      "STDERR:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark_HYBRID_MLP.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
