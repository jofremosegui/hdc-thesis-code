{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_filters=32, kernel_sizes=[3, 5, 7], dropout=0.1, pooling=\"avg\"):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.pooling = pooling\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(input_size, num_filters, kernel_size=k, padding='same') for k in kernel_sizes\n",
    "        ])\n",
    "        self.batch_norms = nn.ModuleList([nn.BatchNorm1d(num_filters) for _ in kernel_sizes])\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(num_filters * len(kernel_sizes), 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [batch, seq_len, features] -> [batch, features, seq_len]\n",
    "        conv_outputs = []\n",
    "        for conv, bn in zip(self.conv_layers, self.batch_norms):\n",
    "            out = F.relu(bn(conv(x)))\n",
    "            if self.pooling == \"max\":\n",
    "                pooled = F.adaptive_max_pool1d(out, 1).squeeze(2)\n",
    "            else:\n",
    "                pooled = F.adaptive_avg_pool1d(out, 1).squeeze(2)\n",
    "            conv_outputs.append(pooled)\n",
    "        x = torch.cat(conv_outputs, dim=1)\n",
    "        return self.fc(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CNN model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Load the model ---\n",
    "MODEL_PATH = \"results/1D-CNN_DATA_AUGMENTATION/20250528_193914/model_20250528_201856.pt\"\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = CNNClassifier(input_size=INPUT_SHAPE[2])\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ CNN model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "def mock_preprocessing():\n",
    "    # Simulate any windowing or normalization before inference\n",
    "    x = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n",
    "    return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (avg ± std): 0.26 ± 0.07 ms\n",
      "Total Window-to-decision Latency: 0.37 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Benchmark ---\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "\n",
    "sample_input = mock_preprocessing().to(device)\n",
    "for _ in range(10):  # Warm-up\n",
    "    _ = model(sample_input)\n",
    "\n",
    "times, latencies = [], []\n",
    "for _ in range(100):\n",
    "    gc.collect()\n",
    "    input_tensor = mock_preprocessing().to(device)\n",
    "\n",
    "    start_all = time.perf_counter()\n",
    "    _ = model(input_tensor)\n",
    "    end_all = time.perf_counter()\n",
    "\n",
    "    start_infer = time.perf_counter()\n",
    "    _ = model(sample_input)\n",
    "    end_infer = time.perf_counter()\n",
    "\n",
    "    latencies.append((end_all - start_all) * 1000)\n",
    "    times.append((end_infer - start_infer) * 1000)\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "avg_latency = np.mean(latencies)\n",
    "\n",
    "print(f\"Inference Time (avg ± std): {avg_time:.2f} ± {std_time:.2f} ms\")\n",
    "print(f\"Total Window-to-decision Latency: {avg_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 0.03 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "model_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.00 MB\n",
      "Total Memory Usage After Inference: 282.16 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Memory Usage ---\n",
    "sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_input)\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem_mb = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem_mb = mem_after / (1024 ** 2)\n",
    "\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem_mb:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.003 J\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Energy Estimate ---\n",
    "ENERGY_ESTIMATED_WATT = 12\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 3.56 MB\n",
      "Total Memory Usage After Inference: 180.03 MB\n",
      "\n",
      "STDERR:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark_CNN.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
