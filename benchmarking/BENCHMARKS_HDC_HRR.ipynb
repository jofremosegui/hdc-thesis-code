{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/__init__.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/embeddings.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/models.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchhd import models, embeddings\n",
    "\n",
    "class HdcGenericEncoder(nn.Module):\n",
    "    def __init__(self, input_size, out_dimension, ngrams=7, dtype=torch.float32, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.keys = embeddings.Random(input_size, out_dimension, dtype=dtype, device=device, vsa=\"HRR\")\n",
    "        self.motion_embed = embeddings.Level(3000, out_dimension, dtype=dtype, low=-3.0, high=3.0, device=device, vsa=\"HRR\")\n",
    "        self.hr_embed = embeddings.Level(200, out_dimension, dtype=dtype, low=50, high=200, device=device, vsa=\"HRR\")\n",
    "        self.ngrams = ngrams\n",
    "        self.device = device\n",
    "\n",
    "    def batch_generic(self, id, levels, ngram):\n",
    "        batch_size = levels.shape[0]\n",
    "        multiset_list = []\n",
    "        for b in range(batch_size):\n",
    "            level = levels[b]\n",
    "            b_levels = [\n",
    "                torchhd.ngrams(level[0][i : i + ngram], ngram)\n",
    "                for i in range(1, id.shape[0] - ngram + 1)\n",
    "            ]\n",
    "            if len(b_levels) > 0:\n",
    "                b_levels = torch.stack(b_levels)\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id[:-ngram], b_levels)).unsqueeze(0))\n",
    "            else:\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id, level)))\n",
    "        return torch.stack(multiset_list)\n",
    "\n",
    "    def forward(self, channels):\n",
    "        motion = channels[:, :, :self.input_size - 1]\n",
    "        hr = channels[:, :, self.input_size - 1].unsqueeze(-1)\n",
    "        enc_motion = self.motion_embed(motion)\n",
    "        enc_hr = self.hr_embed(hr)\n",
    "        enc = torch.cat([enc_motion, enc_hr], dim=2)\n",
    "        hvs = self.batch_generic(self.keys.weight, enc, self.ngrams)\n",
    "        return torchhd.hard_quantize(torchhd.multiset(hvs))\n",
    "\n",
    "class HdcModel(nn.Module):\n",
    "    def __init__(self, input_size, out_dimension=5000, ngrams=7, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.encoder = HdcGenericEncoder(input_size, out_dimension, ngrams, device=device)\n",
    "        self.centroid = models.Centroid(out_dimension, 2, device=device)\n",
    "\n",
    "    def vector_norm(self, x, p=2, dim=None, keepdim=False):\n",
    "        return torch.pow(torch.sum(torch.abs(x) ** p, dim=dim, keepdim=keepdim), 1 / p)\n",
    "\n",
    "    def normalized_inference(self, input, dot=False):\n",
    "        weight = self.centroid.weight.detach().clone()\n",
    "        norms = self.vector_norm(weight, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "        weight.div_(norms)\n",
    "        return torchhd.functional.dot_similarity(input, weight) if dot else torchhd.functional.cosine_similarity(input, weight)\n",
    "\n",
    "    def binary_hdc_output(self, outputs):\n",
    "        return torch.nn.functional.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        hv = self.encoder(x)\n",
    "        out = self.normalized_inference(hv, dot=True)\n",
    "        return self.binary_hdc_output(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HRR model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Load the model ---\n",
    "# MODEL_PATH = \"results/1D-CNN_DATA_AUGMENTATION/20250528_193914/model_20250528_201856.pt\"\n",
    "# INPUT_SHAPE = (1, 800, 7)\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "# model = CNNClassifier(input_size=INPUT_SHAPE[2])\n",
    "# state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "# model.load_state_dict(state_dict)\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "MODEL_PATH = \"results/HDC_HRR_UNDERSAMPLING/20250430_144301/model_20250430_182301.pt\"\n",
    "\n",
    "device = 'cpu'\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "model = HdcModel(input_size=7, out_dimension=5000, ngrams=7, device=device)\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ HRR model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "def mock_preprocessing():\n",
    "    x = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n",
    "    return torch.tensor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (avg ± std): 10.87 ± 0.63 ms\n",
      "Total Window-to-decision Latency: 10.90 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Benchmark ---\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "\n",
    "sample_input = mock_preprocessing().to(device)\n",
    "for _ in range(10):  # Warm-up\n",
    "    _ = model(sample_input)\n",
    "\n",
    "times, latencies = [], []\n",
    "for _ in range(100):\n",
    "    gc.collect()\n",
    "    input_tensor = mock_preprocessing().to(device)\n",
    "\n",
    "    start_all = time.perf_counter()\n",
    "    _ = model(input_tensor)\n",
    "    end_all = time.perf_counter()\n",
    "\n",
    "    start_infer = time.perf_counter()\n",
    "    _ = model(sample_input)\n",
    "    end_infer = time.perf_counter()\n",
    "\n",
    "    latencies.append((end_all - start_all) * 1000)\n",
    "    times.append((end_infer - start_infer) * 1000)\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "avg_latency = np.mean(latencies)\n",
    "\n",
    "print(f\"Inference Time (avg ± std): {avg_time:.2f} ± {std_time:.2f} ms\")\n",
    "print(f\"Total Window-to-decision Latency: {avg_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 61.21 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "model_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.02 MB\n",
      "Total Memory Usage After Inference: 942.41 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "# --- Memory Usage ---\n",
    "sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_input)\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem_mb = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem_mb = mem_after / (1024 ** 2)\n",
    "\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem_mb:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.130 J\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Energy Estimate ---\n",
    "ENERGY_ESTIMATED_WATT = 12\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 352.77 MB\n",
      "Total Memory Usage After Inference: 882.94 MB\n",
      "\n",
      "STDERR:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark_HDC_HRR.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
