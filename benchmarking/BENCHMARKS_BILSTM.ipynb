{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import psutil\n",
    "# Define the Bi-LSTM model\n",
    "# %%\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=1, dropout=0.3, bidirectional=True, pooling=\"attention\"):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pooling = pooling\n",
    "        self.direction_factor = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        if pooling == \"attention\":\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(hidden_size * self.direction_factor, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "            )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.direction_factor, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def attention_pooling(self, lstm_out):\n",
    "        weights = self.attention(lstm_out)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        return torch.sum(weights * lstm_out, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        if self.pooling == \"attention\":\n",
    "            pooled = self.attention_pooling(lstm_out)\n",
    "        else:\n",
    "            pooled = lstm_out[:, -1, :]\n",
    "        return self.fc(pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully on CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Load the Model\n",
    "\n",
    "# %%\n",
    "MODEL_PATH = \"results/biLSTM_DATA_AUGMENTATION/20250520_183833/model_20250521_024421.pt\"\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = LSTMClassifier(input_size=INPUT_SHAPE[2], pooling=\"attention\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Remove 'module.' prefix if present\n",
    "clean_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "model.load_state_dict(clean_state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model loaded successfully on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "def mock_preprocessing():\n",
    "    # Simulate any windowing or normalization before inference\n",
    "    x = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n",
    "    return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (avg ± std over 100 runs): 16.98 ± 0.29 ms\n",
      "Window-to-decision Latency: 17.27 ms\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Inference + Latency Benchmark (Improved)\n",
    "# -------------------------------\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "\n",
    "sample_input = mock_preprocessing().to(device)\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(10):\n",
    "    _ = model(sample_input)\n",
    "\n",
    "times = []\n",
    "latencies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        gc.collect()  # Clear Python garbage\n",
    "        torch.cuda.empty_cache()  # Safe for CPU use too\n",
    "\n",
    "        # Simulate full latency (windowing + inference)\n",
    "        start_all = timer()\n",
    "        input_tensor = mock_preprocessing().to(device)\n",
    "        _ = model(input_tensor)\n",
    "        end_all = timer()\n",
    "\n",
    "        # Isolated model-only inference\n",
    "        start_infer = timer()\n",
    "        _ = model(sample_input)\n",
    "        end_infer = timer()\n",
    "\n",
    "        latencies.append((end_all - start_all) * 1000)\n",
    "        times.append((end_infer - start_infer) * 1000)\n",
    "\n",
    "# Inference time stats\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "print(f\"Inference Time (avg ± std over 100 runs): {avg_time:.2f} ± {std_time:.2f} ms\")\n",
    "\n",
    "# Total latency (preprocessing + inference)\n",
    "avg_latency = np.mean(latencies)\n",
    "print(f\"Window-to-decision Latency: {avg_latency:.2f} ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 0.70 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "model_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.00 MB\n",
      "Total Memory Usage After Inference: 314.69 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()  # Safe even on CPU\n",
    "\n",
    "# Prepare input\n",
    "sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "\n",
    "# Ensure any pending memory allocation is flushed\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "\n",
    "# Track process\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "    del output  # Explicitly delete output\n",
    "\n",
    "torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem_mb = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem_mb = mem_after / (1024 ** 2)\n",
    "\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem_mb:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.204 J\n"
     ]
    }
   ],
   "source": [
    "ENERGY_ESTIMATED_WATT = 12  # conservative CPU-only active power draw\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT  # in joules\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 11.05 MB\n",
      "Total Memory Usage After Inference: 189.03 MB\n",
      "\n",
      "STDERR:\n",
      " /opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
