{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.2, bidirectional=True, pooling=\"attention\"):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.pooling = pooling\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        if pooling == \"attention\":\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(hidden_size * direction_factor, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * direction_factor, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def attention_pooling(self, gru_out):\n",
    "        weights = self.attention(gru_out)\n",
    "        weights = torch.softmax(weights, dim=1)\n",
    "        return torch.sum(weights * gru_out, dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        if self.pooling == \"attention\":\n",
    "            out = self.attention_pooling(out)\n",
    "        else:\n",
    "            out = out[:, -1, :]\n",
    "        return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GRU model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Load the model ---\n",
    "MODEL_PATH = \"results/GRU_DATA_AUGMENTATION/20250530_233726/model_20250531_195904.pt\"\n",
    "\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = GRUClassifier(input_size=INPUT_SHAPE[2], hidden_size=64, num_layers=2, dropout=0.5, bidirectional=False, pooling=\"attention\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ GRU model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "def mock_preprocessing():\n",
    "    # Simulate any windowing or normalization before inference\n",
    "    x = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n",
    "    return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (avg ± std): 29.50 ± 0.51 ms\n",
      "Total Window-to-decision Latency: 23.86 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Benchmark ---\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "\n",
    "sample_input = mock_preprocessing().to(device)\n",
    "for _ in range(10):  # Warm-up\n",
    "    _ = model(sample_input)\n",
    "\n",
    "times, latencies = [], []\n",
    "for _ in range(100):\n",
    "    gc.collect()\n",
    "    input_tensor = mock_preprocessing().to(device)\n",
    "\n",
    "    start_all = time.perf_counter()\n",
    "    _ = model(input_tensor)\n",
    "    end_all = time.perf_counter()\n",
    "\n",
    "    start_infer = time.perf_counter()\n",
    "    _ = model(sample_input)\n",
    "    end_infer = time.perf_counter()\n",
    "\n",
    "    latencies.append((end_all - start_all) * 1000)\n",
    "    times.append((end_infer - start_infer) * 1000)\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "avg_latency = np.mean(latencies)\n",
    "\n",
    "print(f\"Inference Time (avg ± std): {avg_time:.2f} ± {std_time:.2f} ms\")\n",
    "print(f\"Total Window-to-decision Latency: {avg_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 0.18 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "model_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.00 MB\n",
      "Total Memory Usage After Inference: 282.16 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Memory Usage ---\n",
    "sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_input)\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem_mb = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem_mb = mem_after / (1024 ** 2)\n",
    "\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem_mb:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.354 J\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Energy Estimate ---\n",
    "ENERGY_ESTIMATED_WATT = 12\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 5.33 MB\n",
      "Total Memory Usage After Inference: 181.05 MB\n",
      "\n",
      "STDERR:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark_GRU.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
