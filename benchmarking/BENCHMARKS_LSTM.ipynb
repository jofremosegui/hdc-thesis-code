{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Benchmarking Script\n",
    "\n",
    "This script evaluates inference time, model size, and memory usage for a trained PyTorch model under deployment-like CPU conditions.\n",
    "\n",
    "## What it measures\n",
    "- Inference time (average over 100 CPU runs)\n",
    "- Model size (`.pt` file saved via `torch.save`)\n",
    "- RAM usage during execution (`psutil`)\n",
    "\n",
    "## Usage\n",
    "1. Set `MODEL_PATH` to your saved model.\n",
    "2. Define `INPUT_SHAPE` as used during training (e.g., `(1, 800, 7)`).\n",
    "3. Run the script. Results are printed to the console.\n",
    "\n",
    "All benchmarks are performed on CPU to reflect edge deployment scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        dropout=0.2,\n",
    "        bidirectional=True,\n",
    "        pooling=\"attention\",\n",
    "    ):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pooling = pooling\n",
    "\n",
    "        # bidirectionalの場合、出力の次元数が2倍になる\n",
    "        self.direction_factor = 2 if bidirectional else 1\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Attention層（pooling='attention'の場合に使用）\n",
    "        if pooling == \"attention\":\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(hidden_size * self.direction_factor, hidden_size),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_size, 1),\n",
    "            )\n",
    "\n",
    "        # 全結合層の入力サイズを調整\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * self.direction_factor, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def attention_pooling(self, lstm_out):\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        attended_out = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        return attended_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pooled_output = lstm_out[:, -1, :]\n",
    "\n",
    "        return self.fc(pooled_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LSTM model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Load the model ---\n",
    "MODEL_PATH = \"results/LSTM_DATA_AUGMENTATION/20250601_124653/model_20250601_143308.pt\"\n",
    "\n",
    "INPUT_SHAPE = (1, 800, 7)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = LSTMClassifier(input_size=INPUT_SHAPE[2], hidden_size=64, num_layers=2, dropout=0.5, bidirectional=False, pooling=\"attention\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ LSTM model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Dummy preprocessing function\n",
    "# -------------------------------\n",
    "def mock_preprocessing():\n",
    "    # Simulate any windowing or normalization before inference\n",
    "    x = np.random.randn(*INPUT_SHAPE).astype(np.float32)\n",
    "    return torch.tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (avg ± std): 25.61 ± 0.35 ms\n",
      "Total Window-to-decision Latency: 20.26 ms\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Benchmark ---\n",
    "from timeit import default_timer as timer\n",
    "import gc\n",
    "\n",
    "sample_input = mock_preprocessing().to(device)\n",
    "for _ in range(10):  # Warm-up\n",
    "    _ = model(sample_input)\n",
    "\n",
    "times, latencies = [], []\n",
    "for _ in range(100):\n",
    "    gc.collect()\n",
    "    input_tensor = mock_preprocessing().to(device)\n",
    "\n",
    "    start_all = time.perf_counter()\n",
    "    _ = model(input_tensor)\n",
    "    end_all = time.perf_counter()\n",
    "\n",
    "    start_infer = time.perf_counter()\n",
    "    _ = model(sample_input)\n",
    "    end_infer = time.perf_counter()\n",
    "\n",
    "    latencies.append((end_all - start_all) * 1000)\n",
    "    times.append((end_infer - start_infer) * 1000)\n",
    "\n",
    "avg_time = np.mean(times)\n",
    "std_time = np.std(times)\n",
    "avg_latency = np.mean(latencies)\n",
    "\n",
    "print(f\"Inference Time (avg ± std): {avg_time:.2f} ± {std_time:.2f} ms\")\n",
    "print(f\"Total Window-to-decision Latency: {avg_latency:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 0.23 MB\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Model Size\n",
    "# -------------------------------\n",
    "model_size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ Memory Used During Inference: 0.31 MB\n",
      "Total Memory Usage After Inference: 302.80 MB\n"
     ]
    }
   ],
   "source": [
    "# --- Memory Usage ---\n",
    "sample_input = torch.randn(INPUT_SHAPE).to(device)\n",
    "process = psutil.Process(os.getpid())\n",
    "mem_before = process.memory_info().rss\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_input)\n",
    "\n",
    "mem_after = process.memory_info().rss\n",
    "delta_mem_mb = (mem_after - mem_before) / (1024 ** 2)\n",
    "total_mem_mb = mem_after / (1024 ** 2)\n",
    "\n",
    "print(f\"Δ Memory Used During Inference: {delta_mem_mb:.2f} MB\")\n",
    "print(f\"Total Memory Usage After Inference: {total_mem_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Energy per Inference: 0.307 J\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# --- Energy Estimate ---\n",
    "ENERGY_ESTIMATED_WATT = 12\n",
    "inference_energy = (avg_time / 1000) * ENERGY_ESTIMATED_WATT\n",
    "print(f\"Estimated Energy per Inference: {inference_energy:.3f} J\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT:\n",
      " Δ Memory Used During Inference: 6.45 MB\n",
      "Total Memory Usage After Inference: 181.88 MB\n",
      "\n",
      "STDERR:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"memory_benchmark_GRU.py\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True  # decode bytes to string\n",
    ")\n",
    "\n",
    "print(\"STDOUT:\\n\", result.stdout)\n",
    "print(\"STDERR:\\n\", result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
