{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDC model training notebook \n",
    "\n",
    "Steps:\n",
    "- load raw data \n",
    "- generate configs (hyper parameter)\n",
    "- do cross validation to find the best configs (hyper parameters)\n",
    "- train the model with the best hyperparameters in entire data from cross validation step \n",
    "- evaluate the model with test data (not included in cross validation train/val set)\n",
    "- save the model and result\n",
    "\n",
    "Note:\n",
    "in this notebook, the final performance of the model is evaluated with test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='HDC_statistical_v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1732869081399
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Clear any cached torchhd (in case it was already imported)\n",
    "sys.modules.pop(\"torchhd\", None)\n",
    "\n",
    "# Insert path to your local torchhd *before* importing it\n",
    "sys.path.insert(0, os.path.abspath(\"/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd\"))\n",
    "import torchhd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(models.Centroid, \"add_adjust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Feature Columns for Statistical Data ===\n",
    "STAT_FEATURE_COLUMNS = [\n",
    "    # Accelerometer stats\n",
    "    \"ZVALUEX_acc_mean\", \"ZVALUEX_acc_std\", \"ZVALUEX_acc_min\", \"ZVALUEX_acc_max\", \"ZVALUEX_acc_median\", \"ZVALUEX_acc_range\", \"ZVALUEX_acc_iqr\", \"ZVALUEX_acc_skew\", \"ZVALUEX_acc_kurtosis\",\n",
    "    \"ZVALUEY_acc_mean\", \"ZVALUEY_acc_std\", \"ZVALUEY_acc_min\", \"ZVALUEY_acc_max\", \"ZVALUEY_acc_median\", \"ZVALUEY_acc_range\", \"ZVALUEY_acc_iqr\", \"ZVALUEY_acc_skew\", \"ZVALUEY_acc_kurtosis\",\n",
    "    \"ZVALUEZ_acc_mean\", \"ZVALUEZ_acc_std\", \"ZVALUEZ_acc_min\", \"ZVALUEZ_acc_max\", \"ZVALUEZ_acc_median\", \"ZVALUEZ_acc_range\", \"ZVALUEZ_acc_iqr\", \"ZVALUEZ_acc_skew\", \"ZVALUEZ_acc_kurtosis\",\n",
    "    \n",
    "    # Gyroscope stats\n",
    "    \"ZVALUEX_gyro_mean\", \"ZVALUEX_gyro_std\", \"ZVALUEX_gyro_min\", \"ZVALUEX_gyro_max\", \"ZVALUEX_gyro_median\", \"ZVALUEX_gyro_range\", \"ZVALUEX_gyro_iqr\", \"ZVALUEX_gyro_skew\", \"ZVALUEX_gyro_kurtosis\",\n",
    "    \"ZVALUEY_gyro_mean\", \"ZVALUEY_gyro_std\", \"ZVALUEY_gyro_min\", \"ZVALUEY_gyro_max\", \"ZVALUEY_gyro_median\", \"ZVALUEY_gyro_range\", \"ZVALUEY_gyro_iqr\", \"ZVALUEY_gyro_skew\", \"ZVALUEY_gyro_kurtosis\",\n",
    "    \"ZVALUEZ_gyro_mean\", \"ZVALUEZ_gyro_std\", \"ZVALUEZ_gyro_min\", \"ZVALUEZ_gyro_max\", \"ZVALUEZ_gyro_median\", \"ZVALUEZ_gyro_range\", \"ZVALUEZ_gyro_iqr\", \"ZVALUEZ_gyro_skew\", \"ZVALUEZ_gyro_kurtosis\",\n",
    "    \n",
    "    # Heart rate stats\n",
    "    \"ZHEARTRATE_mean\", \"ZHEARTRATE_std\", \"ZHEARTRATE_min\", \"ZHEARTRATE_max\", \"ZHEARTRATE_median\", \"ZHEARTRATE_range\", \"ZHEARTRATE_iqr\", \"ZHEARTRATE_skew\", \"ZHEARTRATE_kurtosis\"\n",
    "]\n",
    "\n",
    "# === All columns to load from CSV ===\n",
    "RAW_COLUMNS = ['user_id', 'session_id', 'tac_flg'] + STAT_FEATURE_COLUMNS\n",
    "\n",
    "# === Threshold and user info ===\n",
    "TAC_THRESHOLD = 35\n",
    "TAC_LEVEL_0 = 0\n",
    "TAC_LEVEL_1 = 1\n",
    "NUM_TAC_LEVELS = 2\n",
    "\n",
    "ALL_USERS = [6, 9, 10, 11, 14, 15, 16, 24, 25, 26, 28, 31]\n",
    "\n",
    "TRAIN_USERS = [\n",
    "    [9, 10, 14, 15, 24, 28, 31],\n",
    "    [10, 11, 6, 31],\n",
    "    [6, 9, 11, 14, 15, 24, 28]\n",
    "]\n",
    "\n",
    "VALID_USERS = [\n",
    "    [11, 6],\n",
    "    [9, 14, 15, 24, 28],\n",
    "    [10, 31]\n",
    "]\n",
    "\n",
    "TEST_USERS = [16, 25, 26]\n",
    "\n",
    "# === Base configuration for HDC model ===\n",
    "BASE_CONFIGS = {\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"window_size\": [1],  # Ignored in statistical input mode\n",
    "    \"ngrams\": [7],\n",
    "    \"hdc_dimension\": 5000,\n",
    "    \"batch_size\": [64],\n",
    "    \"learning_rate\": [2],\n",
    "    \"epochs\": 10,\n",
    "    \"patience\": 5,\n",
    "    \"overlap_ratio\": 0.5,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preprocess_fld=''):\n",
    "    file_paths = sorted(glob(preprocess_fld + './features_statistical_group*.csv'))\n",
    "    df_final = [pl.read_csv(file_path, columns=RAW_COLUMNS) for file_path in file_paths]\n",
    "\n",
    "    # Align columns across all files\n",
    "    columns = df_final[-1].columns\n",
    "    df_final = pl.concat([data_df[columns] for data_df in df_final])\n",
    "\n",
    "    # Filter by known users\n",
    "    df_final = df_final.filter(df_final['user_id'].is_in(ALL_USERS))\n",
    "\n",
    "    # Create globally unique session IDs\n",
    "    df_final = (\n",
    "        df_final.with_columns([\n",
    "            pl.concat_str([\n",
    "                pl.col('user_id').cast(pl.Utf8),\n",
    "                pl.lit('_'),\n",
    "                pl.col('session_id').cast(pl.Utf8)\n",
    "            ]).alias('combined_key')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col('combined_key').rank(method='dense').cast(pl.Int32).alias('session_id')\n",
    "        ])\n",
    "        .drop('combined_key')\n",
    "    )\n",
    "\n",
    "    return df_final.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalFeatureDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.features[idx]),\n",
    "            torch.FloatTensor([self.labels[idx]]),\n",
    "        )\n",
    "\n",
    "\n",
    "class HdcStatisticalEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, out_dimension: int, dtype=torch.float32, device: str = \"cpu\"):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_dimension = out_dimension\n",
    "        self.device = device\n",
    "        self.keys = embeddings.Random(input_size, out_dimension, dtype=dtype, device=device)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.to(self.device)  # shape: (batch_size, input_size)\n",
    "\n",
    "        # Expand x to shape (batch_size, input_size, 1)\n",
    "        x_expanded = x.unsqueeze(-1)  # shape: (B, I, 1)\n",
    "\n",
    "        # Expand keys to shape (1, input_size, out_dimension)\n",
    "        keys_expanded = self.keys.weight.unsqueeze(0)  # shape: (1, I, D)\n",
    "\n",
    "        # Element-wise multiply: (B, I, 1) * (1, I, D) => (B, I, D)\n",
    "        bound = x_expanded * keys_expanded\n",
    "\n",
    "        # Multiset across input dimension\n",
    "        hv = torchhd.multiset(bound)  # shape: (B, D)\n",
    "\n",
    "        return torchhd.hard_quantize(hv)\n",
    "\n",
    "\n",
    "class HdcModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        out_dimension: int,\n",
    "        ngrams: int = 1,  # Not used in statistical encoder, but kept for config compatibility\n",
    "        dtype=torch.float32,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = HdcStatisticalEncoder(input_size, out_dimension, dtype=dtype, device=device)\n",
    "        self.centroid = models.Centroid(out_dimension, NUM_TAC_LEVELS, dtype=dtype, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, x: torch.Tensor, y: torch.Tensor, lr: float):\n",
    "        hv = self.encoder(x)\n",
    "        labels = y.to(dtype=torch.int64)\n",
    "        for i in range(len(hv)):\n",
    "            self.centroid.add_adjust(hv[i].unsqueeze(0), labels[i], lr=lr)\n",
    "\n",
    "    def adjust_reset(self):\n",
    "        self.centroid.adjust_reset()\n",
    "\n",
    "    def vector_norm(self, x, p=2, dim=None, keepdim=False):\n",
    "        return torch.pow(torch.sum(torch.abs(x) ** p, dim=dim, keepdim=keepdim), 1 / p)\n",
    "\n",
    "    def normalized_inference(self, input: torch.Tensor, dot: bool = False):\n",
    "        weight = self.centroid.weight.detach().clone()\n",
    "        norms = self.vector_norm(weight, p=2, dim=1, keepdim=True)\n",
    "        norms.clamp_(min=1e-12)\n",
    "        weight.div_(norms)\n",
    "        return torchhd.functional.dot_similarity(input, weight) if dot else torchhd.functional.cosine_similarity(input, weight)\n",
    "\n",
    "    def binary_hdc_output(self, outputs):\n",
    "        return F.softmax(outputs, dim=1)[:, 1]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        hv = self.encoder(x)\n",
    "        output = self.normalized_inference(hv, True)\n",
    "        return output[:, 1]  # raw logits for class 1\n",
    "\n",
    "\n",
    "\n",
    "def prepare_statistical_features(df, feature_columns):\n",
    "    X = df[feature_columns].values\n",
    "    y = df[\"tac_flg\"].values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    val_preds = np.array(val_preds)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    val_prauc = average_precision_score(val_labels, val_preds)\n",
    "    val_rocauc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss / len(valid_loader),\n",
    "        \"pr_auc\": val_prauc,\n",
    "        \"roc_auc\": val_rocauc,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    lr,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    patience=5,\n",
    "):\n",
    "    # Compute class weights (favoring minority class: drunk = 1)\n",
    "    all_labels = torch.cat([y for _, y in train_loader], dim=0).squeeze()\n",
    "    num_neg = (all_labels == 0).sum().item()  # sober\n",
    "    num_pos = (all_labels == 1).sum().item()  # drunk\n",
    "    pos_weight_val = num_neg / max(num_pos, 1)\n",
    "    pos_weight = torch.tensor([pos_weight_val], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "\n",
    "    # Use weighted BCE loss\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    best_val_prauc = 0\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1} / {epochs}\")\n",
    "        model.train()\n",
    "        loss_total = 0\n",
    "        for X, y in train_loader:\n",
    "            model.add(X.to(device), y.to(device), lr)\n",
    "            logits = model(X.to(device))  # raw logits\n",
    "            loss = criterion(logits, y.to(device).squeeze(-1))\n",
    "            loss_total += loss.item()\n",
    "\n",
    "        model.adjust_reset()\n",
    "\n",
    "        if valid_loader:\n",
    "            val_preds, val_labels = inference_dataset(model, valid_loader, device)\n",
    "            val_prauc = average_precision_score(val_labels, val_preds)\n",
    "\n",
    "            if val_prauc >= best_val_prauc:\n",
    "                best_val_prauc = val_prauc\n",
    "                best_state = model.state_dict()\n",
    "                best_epoch = epoch\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model, None, best_epoch\n",
    "\n",
    "\n",
    "\n",
    "def inference_dataset(model, data_loader, device, pred_threshold=None):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            logits = model(batch_X)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "            labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "    return np.array(predictions), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "def performance_calculation(pred_prob, gt_label, threshold=None):\n",
    "    gt_label = gt_label.astype(int)\n",
    "    if threshold is None:\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        for t in thresholds:\n",
    "            temp_pred = (pred_prob >= t).astype(int)\n",
    "            temp_f1 = f1_score(gt_label, temp_pred)\n",
    "            if temp_f1 > best_f1:\n",
    "                best_f1 = temp_f1\n",
    "                best_threshold = t\n",
    "        threshold = best_threshold\n",
    "\n",
    "    pred_label = (pred_prob >= threshold).astype(int)\n",
    "    return (\n",
    "        roc_auc_score(gt_label, pred_prob),\n",
    "        average_precision_score(gt_label, pred_prob),\n",
    "        accuracy_score(gt_label, pred_label),\n",
    "        accuracy_score(gt_label[gt_label == 0], pred_label[gt_label == 0]),\n",
    "        accuracy_score(gt_label[gt_label == 1], pred_label[gt_label == 1]),\n",
    "        f1_score(gt_label, pred_label),\n",
    "        threshold\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_configs(base_config):\n",
    "    list_keys = [key for key, value in base_config.items() if isinstance(value, list)]\n",
    "    if not list_keys:\n",
    "        return [base_config]\n",
    "\n",
    "    key = list_keys[0]\n",
    "    values = base_config[key]\n",
    "    configs = []\n",
    "\n",
    "    for value in values:\n",
    "        new_config = base_config.copy()\n",
    "        new_config[key] = value\n",
    "        configs.extend(generate_configs(new_config))\n",
    "\n",
    "    return configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, save_folder, train_preds, train_gt_labels, test_preds, test_gt_labels, metrics, config):\n",
    "    \"\"\"\n",
    "    Save model, predictions, ground truth, metrics, and model structure to the specified folder.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        save_folder: Folder path to save results\n",
    "        train_preds: Training predictions\n",
    "        train_gt_labels: Training ground truth labels\n",
    "        test_preds: Test predictions\n",
    "        test_gt_labels: Test ground truth labels\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        config: Model configuration dictionary\n",
    "    \"\"\"\n",
    "    # Create a timestamp for the save files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_folder, f\"model_{timestamp}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save model architecture as text\n",
    "    model_structure_path = os.path.join(save_folder, f\"model_structure_{timestamp}.txt\")\n",
    "    with open(model_structure_path, 'w') as f:\n",
    "        f.write(str(model))\n",
    "    \n",
    "    # Save predictions and ground truth\n",
    "    predictions_data = {\n",
    "        'train_predictions': train_preds.tolist() if isinstance(train_preds, np.ndarray) else train_preds,\n",
    "        'train_ground_truth': train_gt_labels.tolist() if isinstance(train_gt_labels, np.ndarray) else train_gt_labels,\n",
    "        'test_predictions': test_preds.tolist() if isinstance(test_preds, np.ndarray) else test_preds,\n",
    "        'test_ground_truth': test_gt_labels.tolist() if isinstance(test_gt_labels, np.ndarray) else test_gt_labels\n",
    "    }\n",
    "    pred_path = os.path.join(save_folder, f\"predictions_{timestamp}.pkl\")\n",
    "    with open(pred_path, 'wb') as f:\n",
    "        pickle.dump(predictions_data, f)\n",
    "    \n",
    "    # Save all metrics\n",
    "    metrics_path = os.path.join(save_folder, f\"metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Save the configuration\n",
    "    config_path = os.path.join(save_folder, f\"config_{timestamp}.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Model, predictions, ground truth, and metrics saved in {save_folder}\")\n",
    "\n",
    "\n",
    "def train_and_eval_final_model(best_config, best_threshold, df):\n",
    "    print('\\nBEGIN TRAIN AND EVALUATION FINAL MODEL\\n')\n",
    "    feature_columns = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        \"ZHEARTRATE\",\n",
    "    ]\n",
    "\n",
    "    # Hyper parameter loading\n",
    "    device = best_config['device'] \n",
    "    window_size = best_config['window_size']\n",
    "    batch_size = best_config['batch_size']\n",
    "    hdc_dimension = best_config['hdc_dimension']\n",
    "    ngrams = best_config['ngrams']\n",
    "    learning_rate = best_config['learning_rate']\n",
    "    epochs = best_config['epochs']\n",
    "    patience = best_config['patience']\n",
    "    runtime_log_fld = best_config['runtime_log_fld']\n",
    "    overlap_ratio = best_config['overlap_ratio']\n",
    "    \n",
    "    train_user = list(set(ALL_USERS)- set(TEST_USERS))\n",
    "    test_user = TEST_USERS\n",
    "        \n",
    "    train_data = df[df['user_id'].isin(train_user)]\n",
    "    test_data = df[df['user_id'].isin(test_user)]\n",
    "\n",
    "\n",
    "    # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    scaler = StandardScaler()\n",
    "    train_data[STAT_FEATURE_COLUMNS] = scaler.fit_transform(train_data[STAT_FEATURE_COLUMNS])\n",
    "    test_data[STAT_FEATURE_COLUMNS] = scaler.transform(test_data[STAT_FEATURE_COLUMNS])\n",
    "\n",
    "\n",
    "    # Prepare statistical features\n",
    "    feature_columns = [\n",
    "        col for col in df.columns if col.startswith(\"ZVALUE\") or col.startswith(\"ZHEARTRATE_\")\n",
    "    ]\n",
    "    input_size = len(feature_columns)\n",
    "    X_train = train_data[feature_columns].values\n",
    "    y_train = train_data[\"tac_flg\"].values\n",
    "    X_test = test_data[feature_columns].values\n",
    "    y_test = test_data[\"tac_flg\"].values\n",
    "\n",
    "    train_dataset = StatisticalFeatureDataset(X_train, y_train)\n",
    "    test_dataset = StatisticalFeatureDataset(X_test, y_test)\n",
    "\n",
    "    print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "    print(f\"Users in test:{set(test_data['user_id'])}\")\n",
    "    print(f\"Number of windows for training:{len(X_train)}\")\n",
    "    print(f\"Number of windows for testing:{len(X_test)}\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 02. Train model (set patience to ensure that the model is trained for the best epoch)\n",
    "    model, training_history, _ = train_model(\n",
    "        model=model, train_loader=train_loader, valid_loader=None, lr=learning_rate, device=device, epochs=epochs\n",
    "    )\n",
    "\n",
    "    # 03. Inference\n",
    "    train_preds, train_gt_labels = inference_dataset(model, train_loader, device)\n",
    "    test_preds, test_gt_labels = inference_dataset(model, test_loader, device)\n",
    "\n",
    "    # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "    train_roc_auc, train_pr_auc, train_accuracy, train_sober_acc, train_drunk_acc, train_f1, train_threshold = performance_calculation(train_preds, train_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Training ROC-AUC: {train_roc_auc:.4f}, PR-AUC: {train_pr_auc:.4f}, Accuracy: {train_accuracy:.4f}, Sober Accuracy: {train_sober_acc:.4f}, Drunk Accuracy: {train_drunk_acc:.4f}, F1: {train_f1:.4f}, Threshold: {train_threshold:.4f}\")\n",
    "    test_roc_auc, test_pr_auc, test_accuracy, test_sober_acc, test_drunk_acc, test_f1, test_threshold = performance_calculation(test_preds, test_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Test ROC-AUC: {test_roc_auc:.4f}, PR-AUC: {test_pr_auc:.4f}, Accuracy: {test_accuracy:.4f}, Sober Accuracy: {test_sober_acc:.4f}, Drunk Accuracy: {test_drunk_acc:.4f}, F1: {test_f1:.4f}, Threshold: {test_threshold:.4f}\")    \n",
    "\n",
    "    # 05. Save model, predictions, ground truth, metrics and model structure\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'roc_auc': train_roc_auc,\n",
    "            'pr_auc': train_pr_auc,\n",
    "            'accuracy': train_accuracy,\n",
    "            'sober_accuracy': train_sober_acc,\n",
    "            'drunk_accuracy': train_drunk_acc,\n",
    "            'f1': train_f1,\n",
    "            'threshold': train_threshold\n",
    "        },\n",
    "        'test': {\n",
    "            'roc_auc': test_roc_auc,\n",
    "            'pr_auc': test_pr_auc,\n",
    "            'accuracy': test_accuracy,\n",
    "            'sober_accuracy': test_sober_acc,\n",
    "            'drunk_accuracy': test_drunk_acc,\n",
    "            'f1': test_f1,\n",
    "            'threshold': test_threshold\n",
    "        },\n",
    "        'config': best_config\n",
    "    }\n",
    "    \n",
    "    # Call the function to save all results\n",
    "    save_model_and_results(\n",
    "        model=model,\n",
    "        save_folder=runtime_log_fld,\n",
    "        train_preds=train_preds,\n",
    "        train_gt_labels=train_gt_labels,\n",
    "        test_preds=test_preds,\n",
    "        test_gt_labels=test_gt_labels,\n",
    "        metrics=metrics,\n",
    "        config=best_config\n",
    "    )\n",
    "    \n",
    "    # refresh GPU\n",
    "    model.to(\"cpu\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_accuracy, test_accuracy, metrics\n",
    "\n",
    "def train_cross_validation(df, all_configs):\n",
    "    print(\"=\"*50 + \"\\nBEGIN CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    best_config = None \n",
    "    best_pr_auc = 0\n",
    "    best_threshold = 0.5 \n",
    "\n",
    "    for config_idx, config in enumerate(all_configs):\n",
    "        print(f\"\\nCONFIG {config_idx}: {config}\\n\")\n",
    "\n",
    "        feature_columns = [\n",
    "            col for col in df.columns if col.startswith(\"ZVALUE\") or col.startswith(\"ZHEARTRATE_\")\n",
    "        ]\n",
    "\n",
    "        # Hyper parameter loading\n",
    "        \n",
    "        device = config['device'] \n",
    "        input_size = len(feature_columns)\n",
    "        batch_size = config['batch_size']\n",
    "        hdc_dimension = config['hdc_dimension']\n",
    "        ngrams = config['ngrams']\n",
    "        learning_rate = config['learning_rate']\n",
    "        epochs = config['epochs']\n",
    "        patience = config['patience']\n",
    "\n",
    "        val_all_preds = []\n",
    "        val_all_gt_labels = []\n",
    "        val_trained_epoch = []\n",
    "\n",
    "        for fold, (train_user, val_user) in enumerate(zip(TRAIN_USERS, VALID_USERS)):\n",
    "            print(\"-\" * 100)\n",
    "            print('FOLD:', fold+1)\n",
    "            print('TRAIN:', train_user)\n",
    "            print('VAL:', val_user)\n",
    "\n",
    "            train_data = df[df['user_id'].isin(train_user)].copy()\n",
    "            val_data = df[df['user_id'].isin(val_user)].copy()\n",
    "\n",
    "            # Prepare features and labels\n",
    "            X_train = train_data[feature_columns].values\n",
    "            y_train = train_data[\"tac_flg\"].values\n",
    "            X_val = val_data[feature_columns].values\n",
    "            y_val = val_data[\"tac_flg\"].values\n",
    "\n",
    "            train_dataset = StatisticalFeatureDataset(X_train, y_train)\n",
    "            val_dataset = StatisticalFeatureDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "            pos_weight = torch.tensor([len(y_train[y_train == 0]) / len(y_train[y_train == 1])]).to(device)\n",
    "            pos_weight = torch.tensor([len(y_train[y_train == 0]) / len(y_train[y_train == 1])]).to(device)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "            model, training_history, train_best_epoch = train_model(\n",
    "                model, train_loader, val_loader, lr=learning_rate, device=device, \n",
    "                epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            val_preds, val_gt_labels = inference_dataset(model, val_loader, device)\n",
    "            val_all_preds.append(val_preds)\n",
    "            val_all_gt_labels.append(val_gt_labels)\n",
    "            val_trained_epoch.append(train_best_epoch)\n",
    "\n",
    "            # refresh GPU\n",
    "            model.to(\"cpu\")\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # Evaluate performance\n",
    "        val_all_preds = np.concatenate(val_all_preds)\n",
    "        val_all_gt_labels = np.concatenate(val_all_gt_labels)\n",
    "        val_roc_auc, val_pr_auc, val_accuracy, val_sober_acc, val_drunk_acc, val_f1, val_threshold = performance_calculation(\n",
    "            val_all_preds, val_all_gt_labels\n",
    "        )\n",
    "\n",
    "        print(f\"Validation ROC-AUC: {val_roc_auc:.4f}, PR-AUC: {val_pr_auc:.4f}, Accuracy: {val_accuracy:.4f}, Sober Accuracy: {val_sober_acc:.4f}, Drunk Accuracy: {val_drunk_acc:.4f}, F1: {val_f1:.4f}, Threshold: {val_threshold:.4f}\")\n",
    "\n",
    "        if val_pr_auc > best_pr_auc:\n",
    "            best_pr_auc = val_pr_auc\n",
    "            best_config = config\n",
    "            best_threshold = val_threshold\n",
    "            best_config_epoch = np.ceil(np.mean(val_trained_epoch)) + 1\n",
    "            print(f\"Updated best config: {best_config}, PR-AUC: {best_pr_auc:.4f}, Threshold: {best_threshold:.4f}, Epoch: {best_config_epoch:.2f}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    best_config['epochs'] = int(best_config_epoch)\n",
    "    best_config['patience'] = int(best_config_epoch)\n",
    "    print(f'Final best config: {best_config}, Final best pr_auc: {best_pr_auc}, Final best threshold: {best_threshold}, Final best epoch: {best_config_epoch}')\n",
    "    print(\"=\"*50 + \"\\nEND CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    return best_config, best_pr_auc, best_threshold\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file_path):\n",
    "    # Configure logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Add file handler\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Create a custom print function\n",
    "    original_print = print\n",
    "    \n",
    "    def custom_print(*args, **kwargs):\n",
    "        # Call original print\n",
    "        # original_print(*args, **kwargs)\n",
    "        # Log the printed content\n",
    "        message = \" \".join(str(arg) for arg in args)\n",
    "        logger.info(f\"PRINT: {message}\")\n",
    "    \n",
    "    # Replace built-in print\n",
    "    import builtins\n",
    "    builtins.print = custom_print\n",
    "    \n",
    "    logger.info(f\"Logging initialized to {os.path.abspath(log_file_path)}\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime():\n",
    "    # 1. Set up logging\n",
    "    runtime_log_fld = f\"results/{MODEL_NAME}/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    if not os.path.exists(runtime_log_fld):\n",
    "        os.makedirs(runtime_log_fld)\n",
    "    logger = setup_logging(f\"{runtime_log_fld}/training.log\")\n",
    "    \n",
    "    # 2. Set up configurations\n",
    "    base_configs = BASE_CONFIGS\n",
    "    base_configs['runtime_log_fld'] = runtime_log_fld   \n",
    "    all_configs = generate_configs(base_configs)\n",
    "    print(f\"Total configurations: {len(all_configs)}\")\n",
    "\n",
    "    # 3. Load statistical feature data\n",
    "    df = load_data()\n",
    "\n",
    "    # 4. Train and evaluate all configurations (cross-validation)\n",
    "    best_config, best_pr_auc, best_threshold = train_cross_validation(df, all_configs)\n",
    "\n",
    "    # 5. Train and evaluate the final model with the best configuration\n",
    "    train_and_eval_final_model(best_config, best_threshold, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
