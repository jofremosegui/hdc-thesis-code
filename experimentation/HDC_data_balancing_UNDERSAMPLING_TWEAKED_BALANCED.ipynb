{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDC model training notebook \n",
    "\n",
    "Steps:\n",
    "- load raw data \n",
    "- generate configs (hyper parameter)\n",
    "- do cross validation to find the best configs (hyper parameters)\n",
    "- train the model with the best hyperparameters in entire data from cross validation step \n",
    "- evaluate the model with test data (not included in cross validation train/val set)\n",
    "- save the model and result\n",
    "\n",
    "Note:\n",
    "in this notebook, the final performance of the model is evaluated with test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='HDC_balanced_UNDERSAMPLING_TWEAKED_BALANCED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "gather": {
     "logged": 1732869081399
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Remove torchhd if already loaded\n",
    "if \"torchhd\" in sys.modules:\n",
    "    del sys.modules[\"torchhd\"]\n",
    "\n",
    "# Point to the actual package folder (the one with __init__.py)\n",
    "sys.path.insert(0, os.path.abspath(\"/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd\"))\n",
    "\n",
    "# Now import\n",
    "import torchhd\n",
    "from torchhd import embeddings, models\n",
    "\n",
    "# Sanity check\n",
    "print(torchhd.__file__)\n",
    "assert hasattr(models.Centroid, \"add_adjust\"), \"Custom torchhd still not loaded correctly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(models.Centroid, \"add_adjust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_COLUMNS = ['user_id', 'ZTIME', 'ZVALUEX_acc', 'ZVALUEY_acc', \n",
    "               'ZVALUEZ_acc', 'ZVALUEX_gyro', 'ZVALUEY_gyro', 'ZVALUEZ_gyro', 'ZHEARTRATE', 'ZAVERAGEHEARTRATE', \n",
    "              'tac (ug/L)', 'tac_flg', 'session_id']\n",
    "\n",
    "TAC_THRESHOLD = 35\n",
    "TAC_LEVEL_0 = 0\n",
    "TAC_LEVEL_1 = 1\n",
    "NUM_TAC_LEVELS = 2\n",
    "\n",
    "ALL_USERS = [ 6,  9, 10, 11, 14, 15, 16, 24, 25, 26, 28, 31]\n",
    "\n",
    "TRAIN_USERS = [[9, 10, 14, 15, 24, 28, 31],\n",
    "[10, 11, 6, 31],\n",
    "[6, 9, 11, 14, 15, 24, 28]]\n",
    "\n",
    "\n",
    "VALID_USERS = [[11, 6],\n",
    "[9, 14, 15, 24, 28],\n",
    "[10, 31]]\n",
    "\n",
    "TEST_USERS = [16,25,26]\n",
    "\n",
    "# base config \n",
    "BASE_CONFIGS = {\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"window_size\": [40*20],\n",
    "    \"ngrams\": [7],\n",
    "    \"hdc_dimension\": 5000,\n",
    "    \"batch_size\": [64],\n",
    "    \"learning_rate\": [2],\n",
    "    \"epochs\": 10,\n",
    "    \"patience\": 5, # early stopping\n",
    "    \"overlap_ratio\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDED TWEAKS\n",
    "def add_noise(seq, level=0.01):\n",
    "    return seq + np.random.normal(0, level, seq.shape)\n",
    "\n",
    "def drift_signal(seq, max_shift=0.1):\n",
    "    drift = np.random.uniform(-max_shift, max_shift, (1, seq.shape[1]))\n",
    "    return seq + drift\n",
    "def time_shift(seq, max_shift=5):\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    return np.roll(seq, shift, axis=0)\n",
    "def mixup(seq1, seq2, alpha=0.3):\n",
    "    return alpha * seq1 + (1 - alpha) * seq2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preprocess_fld='../../Preprocessed_all'):\n",
    "    file_paths = sorted(glob(preprocess_fld + '/after_preprocess_group*.csv'))\n",
    "    df_final = [pl.read_csv(file_path, columns=RAW_COLUMNS) for file_path in file_paths]\n",
    "    columns = df_final[-1].columns\n",
    "    df_final = pl.concat([data_df[columns] for data_df in df_final])\n",
    "    df_final = df_final.filter(df_final['user_id'].is_in(ALL_USERS))\n",
    "\n",
    "    # Rebuild session_id\n",
    "    df_final = (\n",
    "        df_final.with_columns([\n",
    "            pl.concat_str([\n",
    "                pl.col('user_id').cast(pl.Utf8),\n",
    "                pl.lit('_'),\n",
    "                pl.col('session_id')\n",
    "            ]).alias('combined_key')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col('combined_key').rank(method='dense').cast(pl.Int32).alias('session_id')\n",
    "        ])\n",
    "        .drop('combined_key')\n",
    "    )\n",
    "\n",
    "    # Fix: invert labels – 1 = sober → 0, 0 = drunk → 1\n",
    "    df_final = df_final.with_columns(\n",
    "        pl.col('tac_flg').cast(float).map_elements(lambda x: 1.0 - x).alias('tac_flg')\n",
    "    )\n",
    "\n",
    "    return df_final.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, features, labels, window_size=10):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.features[idx]),\n",
    "            torch.FloatTensor([self.labels[idx]]),\n",
    "        )\n",
    "\n",
    "class HdcGenericEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, out_dimension: int, ngrams: int = 5, dtype = torch.float32, device : str = \"cpu\"):\n",
    "        super(HdcGenericEncoder, self).__init__()\n",
    "\n",
    "        #Embeddings for raw data\n",
    "        self.input_size = input_size\n",
    "        self.keys = embeddings.Random(input_size, out_dimension, dtype=dtype, device=device)\n",
    "        self.motion_embed = embeddings.Level(3000, out_dimension, dtype=dtype, \n",
    "                                      low=-3.0, high=3.0, device=device)\n",
    "        self.hr_embed = embeddings.Level(200, out_dimension, dtype=dtype, \n",
    "                                      low=50, high=200, device=device)\n",
    "        self.ngrams = ngrams\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def batch_generic(self, id, levels, ngram):\n",
    "        batch_size = levels.shape[0]\n",
    "        multiset_list = []\n",
    "        for b in range(batch_size):\n",
    "            level = levels[b]\n",
    "            b_levels = [\n",
    "                torchhd.ngrams(level[0][i : i + ngram], ngram)\n",
    "                for i in range(1, id.shape[0] - ngram + 1)\n",
    "            ]\n",
    "            if len(b_levels) > 0:\n",
    "                b_levels = torch.stack(b_levels)\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id[:-ngram], b_levels)).unsqueeze(0))\n",
    "            else:\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id, level)))\n",
    "        return torch.stack(multiset_list)\n",
    "\n",
    "    # Encode window\n",
    "    def forward(self, channels: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, window_size, num_channels = channels.shape\n",
    "        motion_signals = channels[:, :, : self.input_size - 1]\n",
    "        hr_signals = channels[:, :, self.input_size - 1].unsqueeze(-1)\n",
    "        \n",
    "        # Use generic encoder\n",
    "        enc_motion_channels = self.motion_embed(motion_signals)\n",
    "        enc_hr_channel = self.hr_embed(hr_signals)\n",
    "        enc_channels = torch.cat([enc_motion_channels, enc_hr_channel], dim = 2)\n",
    "        sample_hvs = self.batch_generic(\n",
    "            self.keys.weight, enc_channels, self.ngrams\n",
    "        )\n",
    "        sample_hv = torchhd.multiset(sample_hvs)\n",
    "\n",
    "        sample_hv = torchhd.hard_quantize(sample_hv)\n",
    "        \n",
    "        return sample_hv\n",
    "    \n",
    "    \n",
    "class HdcModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        out_dimension: int,\n",
    "        ngrams: int = 5,\n",
    "        dtype=torch.float32,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super(HdcModel, self).__init__()\n",
    "        \n",
    "        self.encoder = HdcGenericEncoder(input_size, out_dimension, ngrams=ngrams, dtype=dtype, device=device)\n",
    "        self.centroid = models.Centroid(\n",
    "                out_dimension,\n",
    "                NUM_TAC_LEVELS,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def add(self, x : torch.Tensor, y : torch.Tensor, lr : float):\n",
    "        hv = self.encoder(x)\n",
    "        labels = y.to(dtype=torch.int64)\n",
    "        for i in range(len(hv)):\n",
    "            #This operations can't be done in batches\n",
    "            self.centroid.add_adjust(\n",
    "                    hv[i].unsqueeze(0), labels[i], lr=lr\n",
    "                )\n",
    "            \n",
    "    def adjust_reset(self):\n",
    "        self.centroid.adjust_reset()\n",
    "        \n",
    "    #Executorch safe (0.6.x)\n",
    "    def vector_norm(self, x, p=2, dim=None, keepdim=False):\n",
    "        return torch.pow(torch.sum(torch.abs(x) ** p, dim=dim, keepdim=keepdim), 1 / p)\n",
    "        \n",
    "    def normalized_inference(self, input: torch.Tensor, dot: bool = False):\n",
    "        normalized_weight = self.centroid.weight.detach().clone()\n",
    "        norms = self.vector_norm(normalized_weight, p=2, dim=1, keepdim=True)\n",
    "        norms.clamp_(min=1e-12)\n",
    "        normalized_weight.div_(norms)\n",
    "\n",
    "        if dot:\n",
    "            return torchhd.functional.dot_similarity(input, normalized_weight)\n",
    "        return torchhd.functional.cosine_similarity(input, normalized_weight)\n",
    "        \n",
    "    def binary_hdc_output(self, outputs):\n",
    "        probs = F.softmax(outputs, dim=1)  # Shape: (batch_size, 2)\n",
    "        return probs[:, 1]  # Extract only class 1 probability\n",
    "        \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        hv = self.encoder(x)\n",
    "        output = self.normalized_inference(hv, True)\n",
    "\n",
    "        return self.binary_hdc_output(output)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def prepare_sequences_undersampled(df, window_size=10, overlap_ratio=0.1, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Generate sequences and apply undersampling to balance class distribution.\n",
    "    \"\"\"\n",
    "    session_user_map = pd.Series(df[\"user_id\"].values, index=df[\"session_id\"]).astype(int).to_dict()\n",
    "    if feature_columns is None:\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "    print(f\"Preparing sequences with undersampling...\")\n",
    "\n",
    "    features = df[feature_columns].values\n",
    "    labels = df[\"tac_flg\"].values\n",
    "    session_ids = df[\"session_id\"].values\n",
    "    unique_sessions = np.unique(session_ids)\n",
    "\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    sequence_user_ids = []\n",
    "\n",
    "    for session_id in tqdm(unique_sessions):\n",
    "        session_mask = session_ids == session_id\n",
    "        session_indices = np.where(session_mask)[0]\n",
    "\n",
    "        if len(session_indices) >= window_size:\n",
    "            for start_idx in range(0, len(session_indices) - window_size + 1, int(window_size * overlap_ratio)):\n",
    "                window_indices = session_indices[start_idx : start_idx + window_size]\n",
    "                if len(window_indices) < window_size:\n",
    "                    continue\n",
    "\n",
    "                sequence = features[window_indices]\n",
    "                label = int(stats.mode(labels[window_indices].astype(int), keepdims=False).mode)\n",
    "\n",
    "                sequences.append(sequence)\n",
    "                sequence_labels.append(label)\n",
    "                sequence_user_ids.append(session_user_map[session_id])\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    sequence_labels = np.array(sequence_labels)\n",
    "    sequence_user_ids = np.array(sequence_user_ids)\n",
    "\n",
    "    # === Undersample the majority class (label == 0)\n",
    "    print(\"Balancing sequences via undersampling...\")\n",
    "\n",
    "    sober_mask = sequence_labels == 0\n",
    "    drunk_mask = sequence_labels == 1\n",
    "\n",
    "    sober_seqs = sequences[sober_mask]\n",
    "    sober_labels = sequence_labels[sober_mask]\n",
    "    sober_users = sequence_user_ids[sober_mask]\n",
    "\n",
    "    drunk_seqs = sequences[drunk_mask]\n",
    "    # def augment_drunk(seqs, labels, users, n_aug=1):\n",
    "    #     new_seqs, new_labels, new_users = [], [], []\n",
    "    #     for i in range(len(seqs)):\n",
    "    #         for _ in range(n_aug):\n",
    "    #             aug_seq = add_noise(seqs[i], level=0.01)\n",
    "    #             new_seqs.append(aug_seq)\n",
    "    #             new_labels.append(labels[i])\n",
    "    #             new_users.append(users[i])\n",
    "    #     return (\n",
    "    #         np.concatenate([seqs, np.stack(new_seqs)]),\n",
    "    #         np.concatenate([labels, new_labels]),\n",
    "    #         np.concatenate([users, new_users]),\n",
    "    #     )\n",
    "     # Augmentation functions\n",
    "    def multi_augment_drunk(seqs, labels, users):\n",
    "        new_seqs, new_labels, new_users = [], [], []\n",
    "\n",
    "        for i in range(len(seqs)):\n",
    "            seq = seqs[i]\n",
    "            uid = users[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            # Augmentation 1: light noise\n",
    "            aug1 = add_noise(seq, level=0.01)\n",
    "            new_seqs.append(aug1)\n",
    "            new_labels.append(label)\n",
    "            new_users.append(uid)\n",
    "\n",
    "            # Augmentation 2: drift or mixup\n",
    "            if np.random.rand() < 0.5:\n",
    "                aug2 = drift_signal(seq, max_shift=0.005)\n",
    "            else:\n",
    "                j = np.random.randint(0, len(seqs))\n",
    "                aug2 = mixup(seq, seqs[j])\n",
    "            new_seqs.append(aug2)\n",
    "            new_labels.append(label)\n",
    "            new_users.append(uid)\n",
    "\n",
    "        return (\n",
    "            np.concatenate([seqs, np.stack(new_seqs)]),\n",
    "            np.concatenate([labels, new_labels]),\n",
    "            np.concatenate([users, new_users]),\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Balance to match sober count\n",
    "    drunk_labels = sequence_labels[drunk_mask]\n",
    "    drunk_users = sequence_user_ids[drunk_mask]\n",
    "    # Determine the majority class count\n",
    "    majority_class_size = max(len(sober_seqs), len(drunk_seqs))\n",
    "\n",
    "    # Augment drunk class to match majority size\n",
    "    drunk_seqs, drunk_labels, drunk_users = multi_augment_drunk(\n",
    "        drunk_seqs, drunk_labels, drunk_users\n",
    "    )\n",
    "\n",
    "\n",
    "    # drunk_labels = sequence_labels[drunk_mask]\n",
    "    # drunk_users = sequence_user_ids[drunk_mask]\n",
    "    \n",
    "    # drunk_seqs, drunk_labels, drunk_users = augment_drunk(drunk_seqs, drunk_labels, drunk_users, n_aug=1)\n",
    "    if len(drunk_seqs) == 0 or len(sober_seqs) == 0:\n",
    "        raise ValueError(\"Insufficient class samples for undersampling.\")\n",
    "\n",
    "    undersample_ratio = 1.5  # or test 1.2, 1.3...\n",
    "    n_samples = min(len(sober_seqs), int(len(drunk_seqs) * undersample_ratio))\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    if len(sober_seqs) > n_samples:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=(len(sober_seqs) - n_samples), random_state=42)\n",
    "        idxs_to_keep, _ = next(sss.split(sober_seqs, sober_users))\n",
    "        sober_seqs_resampled = sober_seqs[idxs_to_keep]\n",
    "        sober_labels_resampled = sober_labels[idxs_to_keep]\n",
    "        sober_users_resampled = sober_users[idxs_to_keep]\n",
    "    else:\n",
    "        # Not enough to subsample, just keep all\n",
    "        sober_seqs_resampled = sober_seqs\n",
    "        sober_labels_resampled = sober_labels\n",
    "        sober_users_resampled = sober_users\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Combine and shuffle\n",
    "    sequences_balanced = np.concatenate([sober_seqs_resampled, drunk_seqs])\n",
    "    labels_balanced = np.concatenate([sober_labels_resampled, drunk_labels])\n",
    "    users_balanced = np.concatenate([sober_users_resampled, drunk_users])\n",
    "\n",
    "    indices = np.arange(len(labels_balanced))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    sequences_balanced = sequences_balanced[indices]\n",
    "    labels_balanced = labels_balanced[indices]\n",
    "    users_balanced = users_balanced[indices]\n",
    "\n",
    "    print(f\"Undersampled sequence shape: {sequences_balanced.shape}\")\n",
    "\n",
    "    return sequences_balanced, labels_balanced, users_balanced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    \"\"\"検証データでのモデル評価\"\"\"\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    val_preds = np.array(val_preds)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    val_prauc = average_precision_score(val_labels, val_preds)\n",
    "    val_rocauc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss / len(valid_loader),\n",
    "        \"pr_auc\": val_prauc,\n",
    "        \"roc_auc\": val_rocauc,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model : HdcModel,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    lr,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    patience=5,\n",
    "):\n",
    "    best_val_prauc = 0\n",
    "    patience_counter = 0\n",
    "    best_train_epoch = 0\n",
    "    best_model_state = None\n",
    "    training_history = []\n",
    "\n",
    "    print(\n",
    "        \"Epoch | Train Loss |  Val Loss  |  Val PR-AUC  |  Val ROC-AUC  |  Epoch Time (s)\"\n",
    "    )\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            model.add(batch_X, batch_y, lr)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        model.adjust_reset()\n",
    "\n",
    "        if valid_loader is None:\n",
    "            epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"time\": epoch_time}\n",
    "            training_history.append(epoch_results)\n",
    "            print(f\"{epoch+1:5d} | {train_loss:.6f} | ------ | ------ | {epoch_time:.2f}\")\n",
    "            continue\n",
    "    \n",
    "        val_metrics = validate_model(model, valid_loader, criterion, device)\n",
    "        epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_metrics[\"loss\"], \n",
    "                         \"val_pr_auc\": val_metrics[\"pr_auc\"], \"val_roc_auc\": val_metrics[\"roc_auc\"], \"time\": epoch_time}\n",
    "        training_history.append(epoch_results)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch+1:5d} | {train_loss:.6f} | {val_metrics['loss']:.6f} | \"\n",
    "            f\"{val_metrics['pr_auc']:.4f} | {val_metrics['roc_auc']:.4f} | \"\n",
    "            f\"{epoch_time:.2f}\"\n",
    "        )\n",
    "        \n",
    "        if val_metrics[\"pr_auc\"] >= best_val_prauc:\n",
    "            best_val_prauc = val_metrics[\"pr_auc\"]\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            best_train_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after epoch {epoch+1}\")\n",
    "            print(f\"Best validation PR-AUC: {best_val_prauc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model, training_history, best_train_epoch\n",
    "\n",
    "\n",
    "def inference_dataset(model, data_loader, device, pred_threshold=None):\n",
    "    \"\"\"evaluation model after a fold training\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    pred_prob = np.array(predictions)\n",
    "    gt_labels = np.array(labels)\n",
    "    return pred_prob, gt_labels\n",
    "\n",
    "\n",
    "def performance_calculation(pred_prob, gt_label, threshold=None):\n",
    "    '''\n",
    "    Calculate the performance of the model\n",
    "    Args:\n",
    "    pred_prob: list, predicted probability\n",
    "    gt_label: list, ground truth label\n",
    "    threshold: float, threshold for binary classification (None if we are evaluating on train data)\n",
    "    '''\n",
    "    if threshold is None:\n",
    "        # Find the optimal threshold by maximizing F1 score\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)  # Test 99 threshold values\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5  # Default if no better threshold is found\n",
    "        \n",
    "        for t in thresholds:\n",
    "            temp_pred = (pred_prob >= t).astype(int)\n",
    "            temp_f1 = (f1_score(gt_label, temp_pred, pos_label=0) + f1_score(gt_label, temp_pred, pos_label=1)) / 2\n",
    "\n",
    "            \n",
    "            if temp_f1 > best_f1:\n",
    "                best_f1 = temp_f1\n",
    "                best_threshold = t\n",
    "        \n",
    "        threshold = best_threshold\n",
    "        \n",
    "    pred_label = (pred_prob >= threshold).astype(int)\n",
    "    roc_auc = roc_auc_score(gt_label, pred_prob)\n",
    "    pr_auc = average_precision_score(gt_label, pred_prob)\n",
    "    accuracy = accuracy_score(gt_label, pred_label)\n",
    "    # Since 0 = drunk and 1 = sober\n",
    "    drunk_acc = accuracy_score(gt_label[gt_label == 0], pred_label[gt_label == 0])\n",
    "    sober_acc = accuracy_score(gt_label[gt_label == 1], pred_label[gt_label == 1])\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label)\n",
    "    return roc_auc, pr_auc, accuracy, sober_acc, drunk_acc, f1, threshold\n",
    "\n",
    "\n",
    "def generate_configs(base_config):\n",
    "    \"\"\"\n",
    "    Generate multiple configurations from a base config.\n",
    "    For any list values in the base config, create a separate config for each list item.\n",
    "    \n",
    "    Args:\n",
    "        base_config (dict): Base configuration with potential list values\n",
    "        \n",
    "    Returns:\n",
    "        list: List of individual configurations\n",
    "    \"\"\"\n",
    "    # Find all keys with list values\n",
    "    list_keys = [key for key, value in base_config.items() if isinstance(value, list)]\n",
    "    \n",
    "    if not list_keys:\n",
    "        # If no list values found, return the original config\n",
    "        return [base_config]\n",
    "    \n",
    "    # Start with the first list key\n",
    "    key = list_keys[0]\n",
    "    values = base_config[key]\n",
    "    \n",
    "    # Generate configurations for each value of the first list key\n",
    "    configs = []\n",
    "    for value in values:\n",
    "        # Create a new config with this specific value\n",
    "        new_config = base_config.copy()\n",
    "        new_config[key] = value\n",
    "        \n",
    "        # Recursively handle any remaining list keys\n",
    "        remaining_configs = generate_configs(new_config)\n",
    "        configs.extend(remaining_configs)\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, save_folder, train_preds, train_gt_labels, test_preds, test_gt_labels, metrics, config):\n",
    "    \"\"\"\n",
    "    Save model, predictions, ground truth, metrics, and model structure to the specified folder.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        save_folder: Folder path to save results\n",
    "        train_preds: Training predictions\n",
    "        train_gt_labels: Training ground truth labels\n",
    "        test_preds: Test predictions\n",
    "        test_gt_labels: Test ground truth labels\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        config: Model configuration dictionary\n",
    "    \"\"\"\n",
    "    # Create a timestamp for the save files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_folder, f\"model_{timestamp}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save model architecture as text\n",
    "    model_structure_path = os.path.join(save_folder, f\"model_structure_{timestamp}.txt\")\n",
    "    with open(model_structure_path, 'w') as f:\n",
    "        f.write(str(model))\n",
    "    \n",
    "    # Save predictions and ground truth\n",
    "    predictions_data = {\n",
    "        'train_predictions': train_preds.tolist() if isinstance(train_preds, np.ndarray) else train_preds,\n",
    "        'train_ground_truth': train_gt_labels.tolist() if isinstance(train_gt_labels, np.ndarray) else train_gt_labels,\n",
    "        'test_predictions': test_preds.tolist() if isinstance(test_preds, np.ndarray) else test_preds,\n",
    "        'test_ground_truth': test_gt_labels.tolist() if isinstance(test_gt_labels, np.ndarray) else test_gt_labels\n",
    "    }\n",
    "    pred_path = os.path.join(save_folder, f\"predictions_{timestamp}.pkl\")\n",
    "    with open(pred_path, 'wb') as f:\n",
    "        pickle.dump(predictions_data, f)\n",
    "    \n",
    "    # Save all metrics\n",
    "    metrics_path = os.path.join(save_folder, f\"metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Save the configuration\n",
    "    config_path = os.path.join(save_folder, f\"config_{timestamp}.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Model, predictions, ground truth, and metrics saved in {save_folder}\")\n",
    "\n",
    "\n",
    "def train_and_eval_final_model(best_config, best_threshold, df):\n",
    "    print('\\nBEGIN TRAIN AND EVALUATION FINAL MODEL\\n')\n",
    "    feature_columns = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        \"ZHEARTRATE\",\n",
    "    ]\n",
    "\n",
    "    # Hyper parameter loading\n",
    "    device = best_config['device'] \n",
    "    window_size = best_config['window_size']\n",
    "    input_size = len(feature_columns)\n",
    "    batch_size = best_config['batch_size']\n",
    "    hdc_dimension = best_config['hdc_dimension']\n",
    "    ngrams = best_config['ngrams']\n",
    "    learning_rate = best_config['learning_rate']\n",
    "    epochs = best_config['epochs']\n",
    "    patience = best_config['patience']\n",
    "    runtime_log_fld = best_config['runtime_log_fld']\n",
    "    overlap_ratio = best_config['overlap_ratio']\n",
    "    \n",
    "    train_user = list(set(ALL_USERS)- set(TEST_USERS))\n",
    "    test_user = TEST_USERS\n",
    "        \n",
    "    train_data = df[df['user_id'].isin(train_user)]\n",
    "    test_data = df[df['user_id'].isin(test_user)]\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    scaler = StandardScaler()\n",
    "    train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "    test_data[columns_to_standardize] = scaler.transform(test_data[columns_to_standardize])\n",
    "\n",
    "    print(\"Preparing sequences...\")\n",
    "    X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "    X_test, y_test, test_user_ids = prepare_sequences_undersampled(test_data, window_size, overlap_ratio)\n",
    "\n",
    "    print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "    print(f\"Users in test:{set(test_data['user_id'])}\")\n",
    "    print(f\"Number of windows for training:{len(X_train)}\")\n",
    "    print(f\"Number of windows for testing:{len(X_test)}\")\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 02. Train model (set patience to ensure that the model is trained for the best epoch)\n",
    "    model, training_history, _ = train_model(\n",
    "        model=model, train_loader=train_loader, valid_loader=None, \n",
    "        criterion=criterion, lr=learning_rate, device=device, epochs=epochs\n",
    "    )\n",
    "\n",
    "    # 03. Inference\n",
    "    train_preds, train_gt_labels = inference_dataset(model, train_loader, device)\n",
    "    test_preds, test_gt_labels = inference_dataset(model, test_loader, device)\n",
    "\n",
    "    # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "    train_roc_auc, train_pr_auc, train_accuracy, train_sober_acc, train_drunk_acc, train_f1, train_threshold = performance_calculation(train_preds, train_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Training ROC-AUC: {train_roc_auc:.4f}, PR-AUC: {train_pr_auc:.4f}, Accuracy: {train_accuracy:.4f}, Sober Accuracy: {train_sober_acc:.4f}, Drunk Accuracy: {train_drunk_acc:.4f}, F1: {train_f1:.4f}, Threshold: {train_threshold:.4f}\")\n",
    "    test_roc_auc, test_pr_auc, test_accuracy, test_sober_acc, test_drunk_acc, test_f1, test_threshold = performance_calculation(test_preds, test_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Test ROC-AUC: {test_roc_auc:.4f}, PR-AUC: {test_pr_auc:.4f}, Accuracy: {test_accuracy:.4f}, Sober Accuracy: {test_sober_acc:.4f}, Drunk Accuracy: {test_drunk_acc:.4f}, F1: {test_f1:.4f}, Threshold: {test_threshold:.4f}\")    \n",
    "\n",
    "    # 05. Save model, predictions, ground truth, metrics and model structure\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'roc_auc': train_roc_auc,\n",
    "            'pr_auc': train_pr_auc,\n",
    "            'accuracy': train_accuracy,\n",
    "            'sober_accuracy': train_sober_acc,\n",
    "            'drunk_accuracy': train_drunk_acc,\n",
    "            'f1': train_f1,\n",
    "            'threshold': train_threshold\n",
    "        },\n",
    "        'test': {\n",
    "            'roc_auc': test_roc_auc,\n",
    "            'pr_auc': test_pr_auc,\n",
    "            'accuracy': test_accuracy,\n",
    "            'sober_accuracy': test_sober_acc,\n",
    "            'drunk_accuracy': test_drunk_acc,\n",
    "            'f1': test_f1,\n",
    "            'threshold': test_threshold\n",
    "        },\n",
    "        'config': best_config\n",
    "    }\n",
    "    \n",
    "    # Call the function to save all results\n",
    "    save_model_and_results(\n",
    "        model=model,\n",
    "        save_folder=runtime_log_fld,\n",
    "        train_preds=train_preds,\n",
    "        train_gt_labels=train_gt_labels,\n",
    "        test_preds=test_preds,\n",
    "        test_gt_labels=test_gt_labels,\n",
    "        metrics=metrics,\n",
    "        config=best_config\n",
    "    )\n",
    "    \n",
    "    # refresh GPU\n",
    "    model.to(\"cpu\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_accuracy, test_accuracy, metrics\n",
    "\n",
    "def train_cross_validation(df, all_configs):\n",
    "    print(\"=\"*50 + \"\\nBEGIN CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    best_config = None \n",
    "    best_pr_auc = 0\n",
    "    best_threshold = 0.5 \n",
    "    for config_idx, config in enumerate(all_configs):\n",
    "        print(f\"\\nCONFIG {config_idx}: {config}\\n\")\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "        # Hyper parameter loading\n",
    "        device = config['device'] \n",
    "        window_size = config['window_size']\n",
    "        input_size = len(feature_columns)\n",
    "        batch_size = config['batch_size']\n",
    "        hdc_dimension = config['hdc_dimension']\n",
    "        ngrams = config['ngrams']\n",
    "        learning_rate = config['learning_rate']\n",
    "        epochs = config['epochs']\n",
    "        overlap_ratio = config['overlap_ratio']\n",
    "        patience = config['patience']\n",
    "\n",
    "        # variables for temperary storing\n",
    "        val_all_preds = []\n",
    "        val_all_gt_labels = []\n",
    "        val_trained_epoch = []\n",
    "\n",
    "        for fold, (train_user, val_user) in enumerate(zip(TRAIN_USERS, VALID_USERS)):\n",
    "\n",
    "            # 01. Prepare data and define model\n",
    "            print(\"-\" * 100)\n",
    "            print('FOLD:', fold+1)\n",
    "            print('TRAIN:', train_user)\n",
    "            print('VAL:', val_user)\n",
    "            \n",
    "            train_data = df[df['user_id'].isin(train_user)].copy()\n",
    "            val_data = df[df['user_id'].isin(val_user)].copy()\n",
    "\n",
    "            # columns will be normalized\n",
    "            columns_to_standardize = [\n",
    "                \"ZVALUEX_acc\",\n",
    "                \"ZVALUEY_acc\",\n",
    "                \"ZVALUEZ_acc\",\n",
    "                \"ZVALUEX_gyro\",\n",
    "                \"ZVALUEY_gyro\",\n",
    "                \"ZVALUEZ_gyro\",\n",
    "                #'ZHEARTRATE'\n",
    "            ]\n",
    "\n",
    "            # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "            scaler = StandardScaler()\n",
    "            train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "            val_data[columns_to_standardize] = scaler.transform(val_data[columns_to_standardize])\n",
    "\n",
    "            print(\"Preparing sequences...\")\n",
    "            X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "            X_val, y_val, val_user_ids = prepare_sequences_undersampled(val_data, window_size, overlap_ratio)\n",
    "\n",
    "            print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "            print(f\"Users in test:{set(val_data['user_id'])}\")\n",
    "            print(f\"Number of windows for training:{len(X_train)}\")\n",
    "            print(f\"Number of windows for testing:{len(X_val)}\")\n",
    "\n",
    "            train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "            val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "            #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # 02. Train model \n",
    "            model, training_history, train_best_epoch = train_model(\n",
    "                model, train_loader, val_loader, criterion, lr=learning_rate, device=device, \n",
    "                epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            # 03. Inference\n",
    "            val_preds, val_gt_labels = inference_dataset(model, val_loader, device)\n",
    "            val_all_preds.append(val_preds)\n",
    "            val_all_gt_labels.append(val_gt_labels)\n",
    "            val_trained_epoch.append(train_best_epoch)\n",
    "\n",
    "            # refresh GPU\n",
    "            model.to(\"cpu\")\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "        val_all_preds = np.concatenate(val_all_preds)\n",
    "        val_all_gt_labels = np.concatenate(val_all_gt_labels)\n",
    "        val_roc_auc, val_pr_auc, val_accuracy, val_sober_acc, val_drunk_acc, val_f1, val_threshold = performance_calculation(val_preds, val_gt_labels)\n",
    "        print(f\"Validation ROC-AUC: {val_roc_auc:.4f}, PR-AUC: {val_pr_auc:.4f}, Accuracy: {val_accuracy:.4f}, Sober Accuracy: {val_sober_acc:.4f}, Drunk Accuracy: {val_drunk_acc:.4f}, F1: {val_f1:.4f}, Threshold: {val_threshold:.4f}\")\n",
    "        \n",
    "        # 05. set up best config\n",
    "        if val_pr_auc > best_pr_auc:\n",
    "            best_pr_auc = val_pr_auc\n",
    "            best_config = config\n",
    "            best_threshold = val_threshold\n",
    "            best_config_epoch = np.ceil(np.mean(val_trained_epoch)) + 1\n",
    "            print(f\"Updated best config: {best_config}, PR-AUC: {best_pr_auc:.4f}, Threshold: {best_threshold:.4f}, Epoch: {best_config_epoch:.2f}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    best_config['epochs'] = int(best_config_epoch)\n",
    "    best_config['patience'] = int(best_config_epoch)\n",
    "    print(f'Final best config: {best_config}, Final best pr_auc: {best_pr_auc}, Final best threshold: {best_threshold}, Final best epoch: {best_config_epoch}')\n",
    "    print(\"=\"*50 + \"\\nEND CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    return best_config, best_pr_auc, best_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file_path):\n",
    "    # Configure logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Add file handler\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Create a custom print function\n",
    "    original_print = print\n",
    "    \n",
    "    def custom_print(*args, **kwargs):\n",
    "        # Call original print\n",
    "        # original_print(*args, **kwargs)\n",
    "        # Log the printed content\n",
    "        message = \" \".join(str(arg) for arg in args)\n",
    "        logger.info(f\"PRINT: {message}\")\n",
    "    \n",
    "    # Replace built-in print\n",
    "    import builtins\n",
    "    builtins.print = custom_print\n",
    "    \n",
    "    logger.info(f\"Logging initialized to {os.path.abspath(log_file_path)}\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime():\n",
    "    # 1.set up logging\n",
    "    runtime_log_fld = f\"results/{MODEL_NAME}/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    if os.path.exists(runtime_log_fld) == False:\n",
    "        os.makedirs(runtime_log_fld)\n",
    "    logger = setup_logging(f\"{runtime_log_fld}/training.log\")\n",
    "    \n",
    "    # 2.set up configurations\n",
    "    base_configs = BASE_CONFIGS\n",
    "    base_configs['runtime_log_fld'] = runtime_log_fld   \n",
    "    all_configs = generate_configs(base_configs)\n",
    "    print(f\"Total configurations: {len(all_configs)}\")\n",
    "\n",
    "    # 3.Load the raw data\n",
    "    df = load_data()\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    # scaler = StandardScaler()\n",
    "    # df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "    # 4. Train and evaluate all configurations\n",
    "    # get the best config of current model \n",
    "    # get the best threshold of drunk or sober based on all fold validation data when using best config\n",
    "    best_config, best_pr_auc, best_threshold = train_cross_validation(df, all_configs)\n",
    "\n",
    "    # 5.Train and evaluate the final model with the best configuration\n",
    "    # train the final model on all data in cross validation and evaluate on test data\n",
    "    # calculate the performance and save the model, metrics and best config \n",
    "    train_and_eval_final_model(best_config, best_threshold, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:03<00:00,  9.73it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 19.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m runtime()\n",
      "Cell \u001b[0;32mIn[103], line 34\u001b[0m, in \u001b[0;36mruntime\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m columns_to_standardize \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZVALUEX_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZVALUEY_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#'ZHEARTRATE'\u001b[39;00m\n\u001b[1;32m     25\u001b[0m ]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# scaler = StandardScaler()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# get the best config of current model \u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# get the best threshold of drunk or sober based on all fold validation data when using best config\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m best_config, best_pr_auc, best_threshold \u001b[38;5;241m=\u001b[39m train_cross_validation(df, all_configs)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 5.Train and evaluate the final model with the best configuration\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# train the final model on all data in cross validation and evaluate on test data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# calculate the performance and save the model, metrics and best config \u001b[39;00m\n\u001b[1;32m     39\u001b[0m train_and_eval_final_model(best_config, best_threshold, df)\n",
      "Cell \u001b[0;32mIn[101], line 255\u001b[0m, in \u001b[0;36mtrain_cross_validation\u001b[0;34m(df, all_configs)\u001b[0m\n\u001b[1;32m    251\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# 02. Train model \u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m model, training_history, train_best_epoch \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m    256\u001b[0m     model, train_loader, val_loader, criterion, lr\u001b[38;5;241m=\u001b[39mlearning_rate, device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[1;32m    257\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, patience\u001b[38;5;241m=\u001b[39mpatience\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# 03. Inference\u001b[39;00m\n\u001b[1;32m    261\u001b[0m val_preds, val_gt_labels \u001b[38;5;241m=\u001b[39m inference_dataset(model, val_loader, device)\n",
      "Cell \u001b[0;32mIn[100], line 350\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, valid_loader, criterion, lr, device, epochs, patience)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_X, batch_y \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    348\u001b[0m     batch_X, batch_y \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 350\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(batch_X, batch_y, lr)\n\u001b[1;32m    351\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(batch_X)\n\u001b[1;32m    352\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[100], line 90\u001b[0m, in \u001b[0;36mHdcModel.add\u001b[0;34m(self, x, y, lr)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\u001b[38;5;28mself\u001b[39m, x : torch\u001b[38;5;241m.\u001b[39mTensor, y : torch\u001b[38;5;241m.\u001b[39mTensor, lr : \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     hv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     91\u001b[0m     labels \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hv)):\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;66;03m#This operations can't be done in batches\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[100], line 58\u001b[0m, in \u001b[0;36mHdcGenericEncoder.forward\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m     56\u001b[0m enc_hr_channel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhr_embed(hr_signals)\n\u001b[1;32m     57\u001b[0m enc_channels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([enc_motion_channels, enc_hr_channel], dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m sample_hvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_generic(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys\u001b[38;5;241m.\u001b[39mweight, enc_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngrams\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m sample_hv \u001b[38;5;241m=\u001b[39m torchhd\u001b[38;5;241m.\u001b[39mmultiset(sample_hvs)\n\u001b[1;32m     63\u001b[0m sample_hv \u001b[38;5;241m=\u001b[39m torchhd\u001b[38;5;241m.\u001b[39mhard_quantize(sample_hv)\n",
      "Cell \u001b[0;32mIn[100], line 45\u001b[0m, in \u001b[0;36mHdcGenericEncoder.batch_generic\u001b[0;34m(self, id, levels, ngram)\u001b[0m\n\u001b[1;32m     43\u001b[0m         multiset_list\u001b[38;5;241m.\u001b[39mappend(torchhd\u001b[38;5;241m.\u001b[39mmultiset(torchhd\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;28mid\u001b[39m[:\u001b[38;5;241m-\u001b[39mngram], b_levels))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         multiset_list\u001b[38;5;241m.\u001b[39mappend(torchhd\u001b[38;5;241m.\u001b[39mmultiset(torchhd\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;28mid\u001b[39m, level)))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(multiset_list)\n",
      "File \u001b[0;32m~/Desktop/TFG/wearbac_experiments/torchhd/torchhd/functional.py:1156\u001b[0m, in \u001b[0;36mmultiset\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Multiset of input hypervectors.\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \n\u001b[1;32m   1131\u001b[0m \u001b[38;5;124;03mBundles all the input hypervectors together.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m ensure_vsa_tensor(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 1156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmultibundle()\n",
      "File \u001b[0;32m~/Desktop/TFG/wearbac_experiments/torchhd/torchhd/tensors/map.py:230\u001b[0m, in \u001b[0;36mMAPTensor.multibundle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmultibundle\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAPTensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    229\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Bundle multiple hypervectors\"\"\"\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Desktop/TFG/wearbac_experiments/torchhd/torchhd/tensors/base.py:77\u001b[0m, in \u001b[0;36mVSATensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m unwrapped_kwargs \u001b[38;5;241m=\u001b[39m {k: unwrap(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Call the original function\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39munwrapped_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munwrapped_kwargs)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# During tracing, avoid wrapping back into VSATensor\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "files = glob(\"results/HDC_balanced_UNDERSAMPLING_TWEAKED/20250420_132903/predictions_20250420_203516.pkl\")\n",
    "assert len(files) > 0, \"No prediction files found!\"\n",
    "file_path = files[0]\n",
    "with open(file_path, \"rb\") as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "\n",
    "# Get raw test predictions and labels\n",
    "test_probs = np.array(predictions[\"test_predictions\"])\n",
    "test_labels = np.array(predictions[\"test_ground_truth\"])\n",
    "\n",
    "# Use same threshold as during training\n",
    "threshold = 0.07  # or dynamically extract it from your saved metrics\n",
    "\n",
    "# Convert to binary predictions\n",
    "test_binary_preds = (test_probs >= threshold).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_binary_preds)\n",
    "\n",
    "# Print it\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Drunk\", \"Predicted Sober\"],\n",
    "            yticklabels=[\"Actual Drunk\", \"Actual Sober\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Extract predictions and labels\n",
    "test_probs = np.array(predictions[\"test_predictions\"])\n",
    "test_labels = np.array(predictions[\"test_ground_truth\"])\n",
    "threshold = predictions.get(\"threshold\", 0.5)\n",
    "\n",
    "# Predicted labels\n",
    "predicted_labels = (test_probs >= threshold).astype(int)\n",
    "\n",
    "# Print the first N prediction rows\n",
    "N = 5\n",
    "print(\"Idx | Prob | Pred_Label | True_Label\")\n",
    "print(\"-\" * 35)\n",
    "for i in range(N):\n",
    "    print(f\"{i:>3} | {test_probs[i]:.3f} | {predicted_labels[i]}          | {int(test_labels[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9503155946731567"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['test_predictions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['test_ground_truth'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count class labels\n",
    "from collections import Counter\n",
    "test_label_counts = Counter(predictions[\"test_ground_truth\"])\n",
    "train_label_counts = Counter(predictions[\"train_ground_truth\"])\n",
    "\n",
    "print(\"Class counts in TEST set:\")\n",
    "for label, count in sorted(test_label_counts.items()):\n",
    "    print(f\"Label {int(label)}: {count}\")\n",
    "\n",
    "print(\"\\nClass counts in TRAIN set:\")\n",
    "for label, count in sorted(train_label_counts.items()):\n",
    "    print(f\"Label {int(label)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 51662, 0.0: 23781})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 4836, 0.0: 1540})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
