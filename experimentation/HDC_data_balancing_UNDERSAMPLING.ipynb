{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDC model training notebook \n",
    "\n",
    "Steps:\n",
    "- load raw data \n",
    "- generate configs (hyper parameter)\n",
    "- do cross validation to find the best configs (hyper parameters)\n",
    "- train the model with the best hyperparameters in entire data from cross validation step \n",
    "- evaluate the model with test data (not included in cross validation train/val set)\n",
    "- save the model and result\n",
    "\n",
    "Note:\n",
    "in this notebook, the final performance of the model is evaluated with test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='HDC_balanced_UNDERSAMPLING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "gather": {
     "logged": 1732869081399
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Remove torchhd if already loaded\n",
    "if \"torchhd\" in sys.modules:\n",
    "    del sys.modules[\"torchhd\"]\n",
    "\n",
    "# Point to the actual package folder (the one with __init__.py)\n",
    "sys.path.insert(0, os.path.abspath(\"/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd\"))\n",
    "\n",
    "# Now import\n",
    "import torchhd\n",
    "from torchhd import embeddings, models\n",
    "\n",
    "# Sanity check\n",
    "print(torchhd.__file__)\n",
    "assert hasattr(models.Centroid, \"add_adjust\"), \"Custom torchhd still not loaded correctly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(models.Centroid, \"add_adjust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_COLUMNS = ['user_id', 'ZTIME', 'ZVALUEX_acc', 'ZVALUEY_acc', \n",
    "               'ZVALUEZ_acc', 'ZVALUEX_gyro', 'ZVALUEY_gyro', 'ZVALUEZ_gyro', 'ZHEARTRATE', 'ZAVERAGEHEARTRATE', \n",
    "              'tac (ug/L)', 'tac_flg', 'session_id']\n",
    "\n",
    "TAC_THRESHOLD = 35\n",
    "TAC_LEVEL_0 = 0\n",
    "TAC_LEVEL_1 = 1\n",
    "NUM_TAC_LEVELS = 2\n",
    "\n",
    "ALL_USERS = [ 6,  9, 10, 11, 14, 15, 16, 24, 25, 26, 28, 31]\n",
    "\n",
    "TRAIN_USERS = [[9, 10, 14, 15, 24, 28, 31],\n",
    "[10, 11, 6, 31],\n",
    "[6, 9, 11, 14, 15, 24, 28]]\n",
    "\n",
    "\n",
    "VALID_USERS = [[11, 6],\n",
    "[9, 14, 15, 24, 28],\n",
    "[10, 31]]\n",
    "\n",
    "TEST_USERS = [16,25,26]\n",
    "\n",
    "# base config \n",
    "BASE_CONFIGS = {\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"window_size\": [40*20],\n",
    "    \"ngrams\": [7],\n",
    "    \"hdc_dimension\": 5000,\n",
    "    \"batch_size\": [64],\n",
    "    \"learning_rate\": [2],\n",
    "    \"epochs\": 10,\n",
    "    \"patience\": 5, # early stopping\n",
    "    \"overlap_ratio\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preprocess_fld='../../Preprocessed_all'):\n",
    "    file_paths = sorted(glob(preprocess_fld + '/after_preprocess_group*.csv'))\n",
    "    df_final = [pl.read_csv(file_path, columns=RAW_COLUMNS) for file_path in file_paths]\n",
    "    # get the same columns from all the dataframes\n",
    "    columns = df_final[-1].columns\n",
    "    # Optionally, you can concatenate the DataFrames into a single DataFrame\n",
    "    df_final = pl.concat([data_df[columns] for data_df in df_final])\n",
    "    # filter user \n",
    "    df_final  = df_final.filter(df_final['user_id'].is_in(ALL_USERS))\n",
    "    # Create new session_id such that it is unique for all users\n",
    "    df_final = (\n",
    "        df_final.with_columns([\n",
    "            pl.concat_str([\n",
    "                pl.col('user_id').cast(pl.Utf8),\n",
    "                pl.lit('_'),\n",
    "                pl.col('session_id')\n",
    "            ]).alias('combined_key')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col('combined_key').rank(method='dense').cast(pl.Int32).alias('session_id')\n",
    "        ])\n",
    "    )\n",
    "    df_final = df_final.drop('combined_key')\n",
    "    df_final = df_final.with_columns(\n",
    "        tac_flg=(df_final['tac (ug/L)'] < TAC_THRESHOLD).cast(float)\n",
    "\n",
    "    )\n",
    "    return df_final.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, features, labels, window_size=10):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.features[idx]),\n",
    "            torch.FloatTensor([self.labels[idx]]),\n",
    "        )\n",
    "\n",
    "class HdcGenericEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, out_dimension: int, ngrams: int = 5, dtype = torch.float32, device : str = \"cpu\"):\n",
    "        super(HdcGenericEncoder, self).__init__()\n",
    "\n",
    "        #Embeddings for raw data\n",
    "        self.input_size = input_size\n",
    "        self.keys = embeddings.Random(input_size, out_dimension, dtype=dtype, device=device)\n",
    "        self.motion_embed = embeddings.Level(3000, out_dimension, dtype=dtype, \n",
    "                                      low=-3.0, high=3.0, device=device)\n",
    "        self.hr_embed = embeddings.Level(200, out_dimension, dtype=dtype, \n",
    "                                      low=50, high=200, device=device)\n",
    "        self.ngrams = ngrams\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def batch_generic(self, id, levels, ngram):\n",
    "        batch_size = levels.shape[0]\n",
    "        multiset_list = []\n",
    "        for b in range(batch_size):\n",
    "            level = levels[b]\n",
    "            b_levels = [\n",
    "                torchhd.ngrams(level[0][i : i + ngram], ngram)\n",
    "                for i in range(1, id.shape[0] - ngram + 1)\n",
    "            ]\n",
    "            if len(b_levels) > 0:\n",
    "                b_levels = torch.stack(b_levels)\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id[:-ngram], b_levels)).unsqueeze(0))\n",
    "            else:\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id, level)))\n",
    "        return torch.stack(multiset_list)\n",
    "\n",
    "    # Encode window\n",
    "    def forward(self, channels: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, window_size, num_channels = channels.shape\n",
    "        motion_signals = channels[:, :, : self.input_size - 1]\n",
    "        hr_signals = channels[:, :, self.input_size - 1].unsqueeze(-1)\n",
    "        \n",
    "        # Use generic encoder\n",
    "        enc_motion_channels = self.motion_embed(motion_signals)\n",
    "        enc_hr_channel = self.hr_embed(hr_signals)\n",
    "        enc_channels = torch.cat([enc_motion_channels, enc_hr_channel], dim = 2)\n",
    "        sample_hvs = self.batch_generic(\n",
    "            self.keys.weight, enc_channels, self.ngrams\n",
    "        )\n",
    "        sample_hv = torchhd.multiset(sample_hvs)\n",
    "\n",
    "        sample_hv = torchhd.hard_quantize(sample_hv)\n",
    "        \n",
    "        return sample_hv\n",
    "    \n",
    "    \n",
    "class HdcModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        out_dimension: int,\n",
    "        ngrams: int = 5,\n",
    "        dtype=torch.float32,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super(HdcModel, self).__init__()\n",
    "        \n",
    "        self.encoder = HdcGenericEncoder(input_size, out_dimension, ngrams=ngrams, dtype=dtype, device=device)\n",
    "        self.centroid = models.Centroid(\n",
    "                out_dimension,\n",
    "                NUM_TAC_LEVELS,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def add(self, x : torch.Tensor, y : torch.Tensor, lr : float):\n",
    "        hv = self.encoder(x)\n",
    "        labels = y.to(dtype=torch.int64)\n",
    "        for i in range(len(hv)):\n",
    "            #This operations can't be done in batches\n",
    "            self.centroid.add_adjust(\n",
    "                    hv[i].unsqueeze(0), labels[i], lr=lr\n",
    "                )\n",
    "            \n",
    "    def adjust_reset(self):\n",
    "        self.centroid.adjust_reset()\n",
    "        \n",
    "    #Executorch safe (0.6.x)\n",
    "    def vector_norm(self, x, p=2, dim=None, keepdim=False):\n",
    "        return torch.pow(torch.sum(torch.abs(x) ** p, dim=dim, keepdim=keepdim), 1 / p)\n",
    "        \n",
    "    def normalized_inference(self, input: torch.Tensor, dot: bool = False):\n",
    "        normalized_weight = self.centroid.weight.detach().clone()\n",
    "        norms = self.vector_norm(normalized_weight, p=2, dim=1, keepdim=True)\n",
    "        norms.clamp_(min=1e-12)\n",
    "        normalized_weight.div_(norms)\n",
    "\n",
    "        if dot:\n",
    "            return torchhd.functional.dot_similarity(input, normalized_weight)\n",
    "        return torchhd.functional.cosine_similarity(input, normalized_weight)\n",
    "        \n",
    "    def binary_hdc_output(self, outputs):\n",
    "        probs = F.softmax(outputs, dim=1)  # Shape: (batch_size, 2)\n",
    "        return probs[:, 1]  # Extract only class 1 probability\n",
    "        \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        hv = self.encoder(x)\n",
    "        output = self.normalized_inference(hv, True)\n",
    "\n",
    "        return self.binary_hdc_output(output)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def prepare_sequences_undersampled(df, window_size=10, overlap_ratio=0.1, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Generate sequences and apply undersampling to balance class distribution.\n",
    "    \"\"\"\n",
    "    session_user_map = pd.Series(df[\"user_id\"].values, index=df[\"session_id\"]).astype(int).to_dict()\n",
    "    if feature_columns is None:\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "    print(f\"Preparing sequences with undersampling...\")\n",
    "\n",
    "    features = df[feature_columns].values\n",
    "    labels = df[\"tac_flg\"].values\n",
    "    session_ids = df[\"session_id\"].values\n",
    "    unique_sessions = np.unique(session_ids)\n",
    "\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    sequence_user_ids = []\n",
    "\n",
    "    for session_id in tqdm(unique_sessions):\n",
    "        session_mask = session_ids == session_id\n",
    "        session_indices = np.where(session_mask)[0]\n",
    "\n",
    "        if len(session_indices) >= window_size:\n",
    "            for start_idx in range(0, len(session_indices) - window_size + 1, int(window_size * overlap_ratio)):\n",
    "                window_indices = session_indices[start_idx : start_idx + window_size]\n",
    "                if len(window_indices) < window_size:\n",
    "                    continue\n",
    "\n",
    "                sequence = features[window_indices]\n",
    "                label = int(stats.mode(labels[window_indices].astype(int), keepdims=False).mode)\n",
    "\n",
    "                sequences.append(sequence)\n",
    "                sequence_labels.append(label)\n",
    "                sequence_user_ids.append(session_user_map[session_id])\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    sequence_labels = np.array(sequence_labels)\n",
    "    sequence_user_ids = np.array(sequence_user_ids)\n",
    "\n",
    "    # === Undersample the majority class (label == 0)\n",
    "    print(\"Balancing sequences via undersampling...\")\n",
    "\n",
    "    sober_mask = sequence_labels == 0\n",
    "    drunk_mask = sequence_labels == 1\n",
    "\n",
    "    sober_seqs = sequences[sober_mask]\n",
    "    sober_labels = sequence_labels[sober_mask]\n",
    "    sober_users = sequence_user_ids[sober_mask]\n",
    "\n",
    "    drunk_seqs = sequences[drunk_mask]\n",
    "    drunk_labels = sequence_labels[drunk_mask]\n",
    "    drunk_users = sequence_user_ids[drunk_mask]\n",
    "\n",
    "    if len(drunk_seqs) == 0 or len(sober_seqs) == 0:\n",
    "        raise ValueError(\"Insufficient class samples for undersampling.\")\n",
    "\n",
    "    undersample_ratio = 1.5  # or test 1.2, 1.3...\n",
    "    n_samples = min(len(sober_seqs), int(len(drunk_seqs) * undersample_ratio))\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    if len(sober_seqs) > n_samples:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=(len(sober_seqs) - n_samples), random_state=42)\n",
    "        idxs_to_keep, _ = next(sss.split(sober_seqs, sober_users))\n",
    "        sober_seqs_resampled = sober_seqs[idxs_to_keep]\n",
    "        sober_labels_resampled = sober_labels[idxs_to_keep]\n",
    "        sober_users_resampled = sober_users[idxs_to_keep]\n",
    "    else:\n",
    "        # Not enough to subsample, just keep all\n",
    "        sober_seqs_resampled = sober_seqs\n",
    "        sober_labels_resampled = sober_labels\n",
    "        sober_users_resampled = sober_users\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Combine and shuffle\n",
    "    sequences_balanced = np.concatenate([sober_seqs_resampled, drunk_seqs])\n",
    "    labels_balanced = np.concatenate([sober_labels_resampled, drunk_labels])\n",
    "    users_balanced = np.concatenate([sober_users_resampled, drunk_users])\n",
    "\n",
    "    indices = np.arange(len(labels_balanced))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    sequences_balanced = sequences_balanced[indices]\n",
    "    labels_balanced = labels_balanced[indices]\n",
    "    users_balanced = users_balanced[indices]\n",
    "\n",
    "    print(f\"Undersampled sequence shape: {sequences_balanced.shape}\")\n",
    "\n",
    "    return sequences_balanced, labels_balanced, users_balanced\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    \"\"\"検証データでのモデル評価\"\"\"\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    val_preds = np.array(val_preds)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    val_prauc = average_precision_score(val_labels, val_preds)\n",
    "    val_rocauc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss / len(valid_loader),\n",
    "        \"pr_auc\": val_prauc,\n",
    "        \"roc_auc\": val_rocauc,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model : HdcModel,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    lr,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    patience=5,\n",
    "):\n",
    "    best_val_prauc = 0\n",
    "    patience_counter = 0\n",
    "    best_train_epoch = 0\n",
    "    best_model_state = None\n",
    "    training_history = []\n",
    "\n",
    "    print(\n",
    "        \"Epoch | Train Loss |  Val Loss  |  Val PR-AUC  |  Val ROC-AUC  |  Epoch Time (s)\"\n",
    "    )\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            model.add(batch_X, batch_y, lr)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        model.adjust_reset()\n",
    "\n",
    "        if valid_loader is None:\n",
    "            epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"time\": epoch_time}\n",
    "            training_history.append(epoch_results)\n",
    "            print(f\"{epoch+1:5d} | {train_loss:.6f} | ------ | ------ | {epoch_time:.2f}\")\n",
    "            continue\n",
    "    \n",
    "        val_metrics = validate_model(model, valid_loader, criterion, device)\n",
    "        epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_metrics[\"loss\"], \n",
    "                         \"val_pr_auc\": val_metrics[\"pr_auc\"], \"val_roc_auc\": val_metrics[\"roc_auc\"], \"time\": epoch_time}\n",
    "        training_history.append(epoch_results)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch+1:5d} | {train_loss:.6f} | {val_metrics['loss']:.6f} | \"\n",
    "            f\"{val_metrics['pr_auc']:.4f} | {val_metrics['roc_auc']:.4f} | \"\n",
    "            f\"{epoch_time:.2f}\"\n",
    "        )\n",
    "        \n",
    "        if val_metrics[\"pr_auc\"] >= best_val_prauc:\n",
    "            best_val_prauc = val_metrics[\"pr_auc\"]\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            best_train_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after epoch {epoch+1}\")\n",
    "            print(f\"Best validation PR-AUC: {best_val_prauc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model, training_history, best_train_epoch\n",
    "\n",
    "\n",
    "def inference_dataset(model, data_loader, device, pred_threshold=None):\n",
    "    \"\"\"evaluation model after a fold training\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    pred_prob = np.array(predictions)\n",
    "    gt_labels = np.array(labels)\n",
    "    return pred_prob, gt_labels\n",
    "\n",
    "\n",
    "def performance_calculation(pred_prob, gt_label, threshold=None):\n",
    "    '''\n",
    "    Calculate the performance of the model\n",
    "    Args:\n",
    "    pred_prob: list, predicted probability\n",
    "    gt_label: list, ground truth label\n",
    "    threshold: float, threshold for binary classification (None if we are evaluating on train data)\n",
    "    '''\n",
    "    if threshold is None:\n",
    "        # Find the optimal threshold by maximizing F1 score\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)  # Test 99 threshold values\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5  # Default if no better threshold is found\n",
    "        \n",
    "        for t in thresholds:\n",
    "            temp_pred = (pred_prob >= t).astype(int)\n",
    "            temp_f1 = (f1_score(gt_label, temp_pred, pos_label=0) + f1_score(gt_label, temp_pred, pos_label=1)) / 2\n",
    "\n",
    "            \n",
    "            if temp_f1 > best_f1:\n",
    "                best_f1 = temp_f1\n",
    "                best_threshold = t\n",
    "        \n",
    "        threshold = best_threshold\n",
    "        \n",
    "    pred_label = (pred_prob >= threshold).astype(int)\n",
    "    roc_auc = roc_auc_score(gt_label, pred_prob)\n",
    "    pr_auc = average_precision_score(gt_label, pred_prob)\n",
    "    accuracy = accuracy_score(gt_label, pred_label)\n",
    "    sober_acc = accuracy_score(gt_label[gt_label == 0], pred_label[gt_label == 0])\n",
    "    drunk_acc = accuracy_score(gt_label[gt_label == 1], pred_label[gt_label == 1])\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label)\n",
    "    return roc_auc, pr_auc, accuracy, sober_acc, drunk_acc, f1, threshold\n",
    "\n",
    "\n",
    "def generate_configs(base_config):\n",
    "    \"\"\"\n",
    "    Generate multiple configurations from a base config.\n",
    "    For any list values in the base config, create a separate config for each list item.\n",
    "    \n",
    "    Args:\n",
    "        base_config (dict): Base configuration with potential list values\n",
    "        \n",
    "    Returns:\n",
    "        list: List of individual configurations\n",
    "    \"\"\"\n",
    "    # Find all keys with list values\n",
    "    list_keys = [key for key, value in base_config.items() if isinstance(value, list)]\n",
    "    \n",
    "    if not list_keys:\n",
    "        # If no list values found, return the original config\n",
    "        return [base_config]\n",
    "    \n",
    "    # Start with the first list key\n",
    "    key = list_keys[0]\n",
    "    values = base_config[key]\n",
    "    \n",
    "    # Generate configurations for each value of the first list key\n",
    "    configs = []\n",
    "    for value in values:\n",
    "        # Create a new config with this specific value\n",
    "        new_config = base_config.copy()\n",
    "        new_config[key] = value\n",
    "        \n",
    "        # Recursively handle any remaining list keys\n",
    "        remaining_configs = generate_configs(new_config)\n",
    "        configs.extend(remaining_configs)\n",
    "    \n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, save_folder, train_preds, train_gt_labels, test_preds, test_gt_labels, metrics, config):\n",
    "    \"\"\"\n",
    "    Save model, predictions, ground truth, metrics, and model structure to the specified folder.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        save_folder: Folder path to save results\n",
    "        train_preds: Training predictions\n",
    "        train_gt_labels: Training ground truth labels\n",
    "        test_preds: Test predictions\n",
    "        test_gt_labels: Test ground truth labels\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        config: Model configuration dictionary\n",
    "    \"\"\"\n",
    "    # Create a timestamp for the save files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_folder, f\"model_{timestamp}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save model architecture as text\n",
    "    model_structure_path = os.path.join(save_folder, f\"model_structure_{timestamp}.txt\")\n",
    "    with open(model_structure_path, 'w') as f:\n",
    "        f.write(str(model))\n",
    "    \n",
    "    # Save predictions and ground truth\n",
    "    predictions_data = {\n",
    "        'train_predictions': train_preds.tolist() if isinstance(train_preds, np.ndarray) else train_preds,\n",
    "        'train_ground_truth': train_gt_labels.tolist() if isinstance(train_gt_labels, np.ndarray) else train_gt_labels,\n",
    "        'test_predictions': test_preds.tolist() if isinstance(test_preds, np.ndarray) else test_preds,\n",
    "        'test_ground_truth': test_gt_labels.tolist() if isinstance(test_gt_labels, np.ndarray) else test_gt_labels\n",
    "    }\n",
    "    pred_path = os.path.join(save_folder, f\"predictions_{timestamp}.pkl\")\n",
    "    with open(pred_path, 'wb') as f:\n",
    "        pickle.dump(predictions_data, f)\n",
    "    \n",
    "    # Save all metrics\n",
    "    metrics_path = os.path.join(save_folder, f\"metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Save the configuration\n",
    "    config_path = os.path.join(save_folder, f\"config_{timestamp}.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Model, predictions, ground truth, and metrics saved in {save_folder}\")\n",
    "\n",
    "\n",
    "def train_and_eval_final_model(best_config, best_threshold, df):\n",
    "    print('\\nBEGIN TRAIN AND EVALUATION FINAL MODEL\\n')\n",
    "    feature_columns = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        \"ZHEARTRATE\",\n",
    "    ]\n",
    "\n",
    "    # Hyper parameter loading\n",
    "    device = best_config['device'] \n",
    "    window_size = best_config['window_size']\n",
    "    input_size = len(feature_columns)\n",
    "    batch_size = best_config['batch_size']\n",
    "    hdc_dimension = best_config['hdc_dimension']\n",
    "    ngrams = best_config['ngrams']\n",
    "    learning_rate = best_config['learning_rate']\n",
    "    epochs = best_config['epochs']\n",
    "    patience = best_config['patience']\n",
    "    runtime_log_fld = best_config['runtime_log_fld']\n",
    "    overlap_ratio = best_config['overlap_ratio']\n",
    "    \n",
    "    train_user = list(set(ALL_USERS)- set(TEST_USERS))\n",
    "    test_user = TEST_USERS\n",
    "        \n",
    "    train_data = df[df['user_id'].isin(train_user)]\n",
    "    test_data = df[df['user_id'].isin(test_user)]\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    scaler = StandardScaler()\n",
    "    train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "    test_data[columns_to_standardize] = scaler.transform(test_data[columns_to_standardize])\n",
    "\n",
    "    print(\"Preparing sequences...\")\n",
    "    X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "    X_test, y_test, test_user_ids = prepare_sequences_undersampled(test_data, window_size, overlap_ratio)\n",
    "\n",
    "    print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "    print(f\"Users in test:{set(test_data['user_id'])}\")\n",
    "    print(f\"Number of windows for training:{len(X_train)}\")\n",
    "    print(f\"Number of windows for testing:{len(X_test)}\")\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 02. Train model (set patience to ensure that the model is trained for the best epoch)\n",
    "    model, training_history, _ = train_model(\n",
    "        model=model, train_loader=train_loader, valid_loader=None, \n",
    "        criterion=criterion, lr=learning_rate, device=device, epochs=epochs\n",
    "    )\n",
    "\n",
    "    # 03. Inference\n",
    "    train_preds, train_gt_labels = inference_dataset(model, train_loader, device)\n",
    "    test_preds, test_gt_labels = inference_dataset(model, test_loader, device)\n",
    "\n",
    "    # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "    train_roc_auc, train_pr_auc, train_accuracy, train_sober_acc, train_drunk_acc, train_f1, train_threshold = performance_calculation(train_preds, train_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Training ROC-AUC: {train_roc_auc:.4f}, PR-AUC: {train_pr_auc:.4f}, Accuracy: {train_accuracy:.4f}, Sober Accuracy: {train_sober_acc:.4f}, Drunk Accuracy: {train_drunk_acc:.4f}, F1: {train_f1:.4f}, Threshold: {train_threshold:.4f}\")\n",
    "    test_roc_auc, test_pr_auc, test_accuracy, test_sober_acc, test_drunk_acc, test_f1, test_threshold = performance_calculation(test_preds, test_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Test ROC-AUC: {test_roc_auc:.4f}, PR-AUC: {test_pr_auc:.4f}, Accuracy: {test_accuracy:.4f}, Sober Accuracy: {test_sober_acc:.4f}, Drunk Accuracy: {test_drunk_acc:.4f}, F1: {test_f1:.4f}, Threshold: {test_threshold:.4f}\")    \n",
    "\n",
    "    # 05. Save model, predictions, ground truth, metrics and model structure\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'roc_auc': train_roc_auc,\n",
    "            'pr_auc': train_pr_auc,\n",
    "            'accuracy': train_accuracy,\n",
    "            'sober_accuracy': train_sober_acc,\n",
    "            'drunk_accuracy': train_drunk_acc,\n",
    "            'f1': train_f1,\n",
    "            'threshold': train_threshold\n",
    "        },\n",
    "        'test': {\n",
    "            'roc_auc': test_roc_auc,\n",
    "            'pr_auc': test_pr_auc,\n",
    "            'accuracy': test_accuracy,\n",
    "            'sober_accuracy': test_sober_acc,\n",
    "            'drunk_accuracy': test_drunk_acc,\n",
    "            'f1': test_f1,\n",
    "            'threshold': test_threshold\n",
    "        },\n",
    "        'config': best_config\n",
    "    }\n",
    "    \n",
    "    # Call the function to save all results\n",
    "    save_model_and_results(\n",
    "        model=model,\n",
    "        save_folder=runtime_log_fld,\n",
    "        train_preds=train_preds,\n",
    "        train_gt_labels=train_gt_labels,\n",
    "        test_preds=test_preds,\n",
    "        test_gt_labels=test_gt_labels,\n",
    "        metrics=metrics,\n",
    "        config=best_config\n",
    "    )\n",
    "    \n",
    "    # refresh GPU\n",
    "    model.to(\"cpu\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_accuracy, test_accuracy, metrics\n",
    "\n",
    "def train_cross_validation(df, all_configs):\n",
    "    print(\"=\"*50 + \"\\nBEGIN CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    best_config = None \n",
    "    best_pr_auc = 0\n",
    "    best_threshold = 0.5 \n",
    "    for config_idx, config in enumerate(all_configs):\n",
    "        print(f\"\\nCONFIG {config_idx}: {config}\\n\")\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "        # Hyper parameter loading\n",
    "        device = config['device'] \n",
    "        window_size = config['window_size']\n",
    "        input_size = len(feature_columns)\n",
    "        batch_size = config['batch_size']\n",
    "        hdc_dimension = config['hdc_dimension']\n",
    "        ngrams = config['ngrams']\n",
    "        learning_rate = config['learning_rate']\n",
    "        epochs = config['epochs']\n",
    "        overlap_ratio = config['overlap_ratio']\n",
    "        patience = config['patience']\n",
    "\n",
    "        # variables for temperary storing\n",
    "        val_all_preds = []\n",
    "        val_all_gt_labels = []\n",
    "        val_trained_epoch = []\n",
    "\n",
    "        for fold, (train_user, val_user) in enumerate(zip(TRAIN_USERS, VALID_USERS)):\n",
    "\n",
    "            # 01. Prepare data and define model\n",
    "            print(\"-\" * 100)\n",
    "            print('FOLD:', fold+1)\n",
    "            print('TRAIN:', train_user)\n",
    "            print('VAL:', val_user)\n",
    "            \n",
    "            train_data = df[df['user_id'].isin(train_user)].copy()\n",
    "            val_data = df[df['user_id'].isin(val_user)].copy()\n",
    "\n",
    "            # columns will be normalized\n",
    "            columns_to_standardize = [\n",
    "                \"ZVALUEX_acc\",\n",
    "                \"ZVALUEY_acc\",\n",
    "                \"ZVALUEZ_acc\",\n",
    "                \"ZVALUEX_gyro\",\n",
    "                \"ZVALUEY_gyro\",\n",
    "                \"ZVALUEZ_gyro\",\n",
    "                #'ZHEARTRATE'\n",
    "            ]\n",
    "\n",
    "            # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "            scaler = StandardScaler()\n",
    "            train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "            val_data[columns_to_standardize] = scaler.transform(val_data[columns_to_standardize])\n",
    "\n",
    "            print(\"Preparing sequences...\")\n",
    "            X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "            X_val, y_val, val_user_ids = prepare_sequences_undersampled(val_data, window_size, overlap_ratio)\n",
    "\n",
    "            print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "            print(f\"Users in test:{set(val_data['user_id'])}\")\n",
    "            print(f\"Number of windows for training:{len(X_train)}\")\n",
    "            print(f\"Number of windows for testing:{len(X_val)}\")\n",
    "\n",
    "            train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "            val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "            #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # 02. Train model \n",
    "            model, training_history, train_best_epoch = train_model(\n",
    "                model, train_loader, val_loader, criterion, lr=learning_rate, device=device, \n",
    "                epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            # 03. Inference\n",
    "            val_preds, val_gt_labels = inference_dataset(model, val_loader, device)\n",
    "            val_all_preds.append(val_preds)\n",
    "            val_all_gt_labels.append(val_gt_labels)\n",
    "            val_trained_epoch.append(train_best_epoch)\n",
    "\n",
    "            # refresh GPU\n",
    "            model.to(\"cpu\")\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "        val_all_preds = np.concatenate(val_all_preds)\n",
    "        val_all_gt_labels = np.concatenate(val_all_gt_labels)\n",
    "        val_roc_auc, val_pr_auc, val_accuracy, val_sober_acc, val_drunk_acc, val_f1, val_threshold = performance_calculation(val_preds, val_gt_labels)\n",
    "        print(f\"Validation ROC-AUC: {val_roc_auc:.4f}, PR-AUC: {val_pr_auc:.4f}, Accuracy: {val_accuracy:.4f}, Sober Accuracy: {val_sober_acc:.4f}, Drunk Accuracy: {val_drunk_acc:.4f}, F1: {val_f1:.4f}, Threshold: {val_threshold:.4f}\")\n",
    "        \n",
    "        # 05. set up best config\n",
    "        if val_pr_auc > best_pr_auc:\n",
    "            best_pr_auc = val_pr_auc\n",
    "            best_config = config\n",
    "            best_threshold = val_threshold\n",
    "            best_config_epoch = np.ceil(np.mean(val_trained_epoch)) + 1\n",
    "            print(f\"Updated best config: {best_config}, PR-AUC: {best_pr_auc:.4f}, Threshold: {best_threshold:.4f}, Epoch: {best_config_epoch:.2f}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    best_config['epochs'] = int(best_config_epoch)\n",
    "    best_config['patience'] = int(best_config_epoch)\n",
    "    print(f'Final best config: {best_config}, Final best pr_auc: {best_pr_auc}, Final best threshold: {best_threshold}, Final best epoch: {best_config_epoch}')\n",
    "    print(\"=\"*50 + \"\\nEND CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    return best_config, best_pr_auc, best_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file_path):\n",
    "    # Configure logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Add file handler\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Create a custom print function\n",
    "    original_print = print\n",
    "    \n",
    "    def custom_print(*args, **kwargs):\n",
    "        # Call original print\n",
    "        # original_print(*args, **kwargs)\n",
    "        # Log the printed content\n",
    "        message = \" \".join(str(arg) for arg in args)\n",
    "        logger.info(f\"PRINT: {message}\")\n",
    "    \n",
    "    # Replace built-in print\n",
    "    import builtins\n",
    "    builtins.print = custom_print\n",
    "    \n",
    "    logger.info(f\"Logging initialized to {os.path.abspath(log_file_path)}\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime():\n",
    "    # 1.set up logging\n",
    "    runtime_log_fld = f\"results/{MODEL_NAME}/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    if os.path.exists(runtime_log_fld) == False:\n",
    "        os.makedirs(runtime_log_fld)\n",
    "    logger = setup_logging(f\"{runtime_log_fld}/training.log\")\n",
    "    \n",
    "    # 2.set up configurations\n",
    "    base_configs = BASE_CONFIGS\n",
    "    base_configs['runtime_log_fld'] = runtime_log_fld   \n",
    "    all_configs = generate_configs(base_configs)\n",
    "    print(f\"Total configurations: {len(all_configs)}\")\n",
    "\n",
    "    # 3.Load the raw data\n",
    "    df = load_data()\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    # scaler = StandardScaler()\n",
    "    # df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "    # 4. Train and evaluate all configurations\n",
    "    # get the best config of current model \n",
    "    # get the best threshold of drunk or sober based on all fold validation data when using best config\n",
    "    best_config, best_pr_auc, best_threshold = train_cross_validation(df, all_configs)\n",
    "\n",
    "    # 5.Train and evaluate the final model with the best configuration\n",
    "    # train the final model on all data in cross validation and evaluate on test data\n",
    "    # calculate the performance and save the model, metrics and best config \n",
    "    train_and_eval_final_model(best_config, best_threshold, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:03<00:00,  9.71it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 19.54it/s]\n",
      "100%|██████████| 15/15 [17:57<00:00, 71.86s/it]\n",
      "100%|██████████| 28/28 [17:45<00:00, 38.04s/it] \n",
      "100%|██████████| 33/33 [00:03<00:00,  8.51it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.14it/s]\n",
      "100%|██████████| 43/43 [00:04<00:00,  9.10it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 20.00it/s]\n"
     ]
    }
   ],
   "source": [
    "runtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "files = glob(\"results/HDC_balanced_UNDERSAMPLING/20250410_105447/predictions_*.pkl\")\n",
    "assert len(files) > 0, \"No prediction files found!\"\n",
    "file_path = files[0]\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "\n",
    "# Get raw test predictions and labels\n",
    "test_probs = np.array(predictions[\"test_predictions\"])\n",
    "test_labels = np.array(predictions[\"test_ground_truth\"])\n",
    "\n",
    "# Use same threshold as during training\n",
    "threshold = 0.07  # or dynamically extract it from your saved metrics\n",
    "\n",
    "# Convert to binary predictions\n",
    "test_binary_preds = (test_probs >= threshold).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_binary_preds)\n",
    "\n",
    "# Print it\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Sober\", \"Predicted Drunk\"],\n",
    "            yticklabels=[\"Actual Sober\", \"Actual Drunk\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
