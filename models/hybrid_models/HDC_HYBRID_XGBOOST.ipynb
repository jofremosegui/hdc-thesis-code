{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "custom": {
     "useCDN": false
    }
   },
   "source": [
    "# HDC model training notebook \n",
    "\n",
    "Steps:\n",
    "- load raw data \n",
    "- generate configs (hyper parameter)\n",
    "- do cross validation to find the best configs (hyper parameters)\n",
    "- train the model with the best hyperparameters in entire data from cross validation step \n",
    "- evaluate the model with test data (not included in cross validation train/val set)\n",
    "- save the model and result\n",
    "\n",
    "Note:\n",
    "in this notebook, the final performance of the model is evaluated with test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME='HDC_LIGHTGBM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1732869081399
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/__init__.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/embeddings.py\n",
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/models.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import random\n",
    "import builtins\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#Use local Executorch compatible copy of TorchHD\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd\"))\n",
    "sys.path.insert(0, os.path.abspath(\"../../../torchhd/torchhd\"))\n",
    "import torchhd\n",
    "from torchhd import embeddings\n",
    "from torchhd import models\n",
    "print(torchhd.__file__) #Check\n",
    "print(embeddings.__file__) #Check\n",
    "print(models.__file__) #Check\n",
    "from typing import Union, Literal\n",
    "import json \n",
    "import pickle\n",
    "# import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import os\n",
    "from glob import glob\n",
    "import polars as pl \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "# Remove torchhd if already loaded\n",
    "if \"torchhd\" in sys.modules:\n",
    "    del sys.modules[\"torchhd\"]\n",
    "\n",
    "# Point to the actual package folder (the one with __init__.py)\n",
    "sys.path.insert(0, os.path.abspath(\"/Users/jofremosegui/Desktop/TFG/wearbac_experiments/torchhd/torchhd\"))\n",
    "\n",
    "# Now import\n",
    "import torchhd\n",
    "from torchhd import embeddings, models\n",
    "\n",
    "# Sanity check\n",
    "print(torchhd.__file__)\n",
    "assert hasattr(models.Centroid, \"add_adjust\"), \"Custom torchhd still not loaded correctly\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(models.Centroid, \"add_adjust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_COLUMNS = ['user_id', 'ZTIME', 'ZVALUEX_acc', 'ZVALUEY_acc', \n",
    "               'ZVALUEZ_acc', 'ZVALUEX_gyro', 'ZVALUEY_gyro', 'ZVALUEZ_gyro', 'ZHEARTRATE', 'ZAVERAGEHEARTRATE', \n",
    "              'tac (ug/L)', 'tac_flg', 'session_id']\n",
    "\n",
    "TAC_THRESHOLD = 35\n",
    "TAC_LEVEL_0 = 0\n",
    "TAC_LEVEL_1 = 1\n",
    "NUM_TAC_LEVELS = 2\n",
    "\n",
    "ALL_USERS = [ 6,  9, 10, 11, 14, 15, 16, 24, 25, 26, 28, 31]\n",
    "\n",
    "TRAIN_USERS = [[9, 10, 14, 15, 24, 28, 31],\n",
    "[10, 11, 6, 31],\n",
    "[6, 9, 11, 14, 15, 24, 28]]\n",
    "\n",
    "\n",
    "VALID_USERS = [[11, 6],\n",
    "[9, 14, 15, 24, 28],\n",
    "[10, 31]]\n",
    "\n",
    "TEST_USERS = [16,25,26]\n",
    "\n",
    "# base config \n",
    "BASE_CONFIGS = {\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"window_size\": [40*20],\n",
    "    \"ngrams\": [7],\n",
    "    \"hdc_dimension\": 5000,\n",
    "    \"batch_size\": [64],\n",
    "    \"learning_rate\": [2],\n",
    "    \"epochs\": 10,\n",
    "    \"patience\": 5, # early stopping\n",
    "    \"overlap_ratio\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADDED TWEAKS\n",
    "def add_noise(seq, level=0.01):\n",
    "    return seq + np.random.normal(0, level, seq.shape)\n",
    "\n",
    "def drift_signal(seq, max_shift=0.1):\n",
    "    drift = np.random.uniform(-max_shift, max_shift, (1, seq.shape[1]))\n",
    "    return seq + drift\n",
    "def time_shift(seq, max_shift=5):\n",
    "    shift = np.random.randint(-max_shift, max_shift)\n",
    "    return np.roll(seq, shift, axis=0)\n",
    "def mixup(seq1, seq2, alpha=0.3):\n",
    "    return alpha * seq1 + (1 - alpha) * seq2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(preprocess_fld='../../Preprocessed_all'):\n",
    "    file_paths = sorted(glob(preprocess_fld + '/after_preprocess_group*.csv'))\n",
    "    df_final = [pl.read_csv(file_path, columns=RAW_COLUMNS) for file_path in file_paths]\n",
    "    columns = df_final[-1].columns\n",
    "    df_final = pl.concat([data_df[columns] for data_df in df_final])\n",
    "    df_final = df_final.filter(df_final['user_id'].is_in(ALL_USERS))\n",
    "\n",
    "    # Rebuild session_id\n",
    "    df_final = (\n",
    "        df_final.with_columns([\n",
    "            pl.concat_str([\n",
    "                pl.col('user_id').cast(pl.Utf8),\n",
    "                pl.lit('_'),\n",
    "                pl.col('session_id')\n",
    "            ]).alias('combined_key')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col('combined_key').rank(method='dense').cast(pl.Int32).alias('session_id')\n",
    "        ])\n",
    "        .drop('combined_key')\n",
    "    )\n",
    "\n",
    "    # # Fix: invert labels – 1 = sober → 0, 0 = drunk → 1\n",
    "    # df_final = df_final.with_columns(\n",
    "    #     pl.col('tac_flg').cast(float).map_elements(lambda x: 1.0 - x).alias('tac_flg')\n",
    "    # )\n",
    "\n",
    "    return df_final.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
    "\n",
    "def extract_hdc_embeddings(encoder, sequences, batch_size=128, device=\"cpu\"):\n",
    "    encoder.eval()\n",
    "    all_hvs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = torch.tensor(sequences[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            hvs = encoder(batch)\n",
    "            all_hvs.append(hvs.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_hvs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, features, labels, window_size=10):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.FloatTensor(self.features[idx]),\n",
    "            torch.FloatTensor([self.labels[idx]]),\n",
    "        )\n",
    "\n",
    "class HdcGenericEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size: int, out_dimension: int, ngrams: int = 5, dtype=torch.float32, device=\"cpu\"):\n",
    "        super(HdcGenericEncoder, self).__init__()\n",
    "\n",
    "        self.cpu_device = torch.device(\"cpu\")  # Force CPU for all TorchHD ops\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.ngrams = ngrams\n",
    "        self.dtype = dtype\n",
    "        self.device = device  # final model output device (e.g., mps or cpu)\n",
    "\n",
    "        self.keys = embeddings.Random(input_size, out_dimension, dtype=dtype, device=self.cpu_device)\n",
    "        self.motion_embed = embeddings.Level(3000, out_dimension, low=-3.0, high=3.0, dtype=dtype, device=self.cpu_device)\n",
    "        self.hr_embed = embeddings.Level(200, out_dimension, low=50, high=200, dtype=dtype, device=self.cpu_device)\n",
    "\n",
    "    def batch_generic(self, id, levels, ngram):\n",
    "        batch_size = levels.shape[0]\n",
    "        multiset_list = []\n",
    "        for b in range(batch_size):\n",
    "            level = levels[b]\n",
    "            b_levels = [\n",
    "                torchhd.ngrams(level[0][i : i + ngram], ngram)\n",
    "                for i in range(1, id.shape[0] - ngram + 1)\n",
    "            ]\n",
    "            if len(b_levels) > 0:\n",
    "                b_levels = torch.stack(b_levels)\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id[:-ngram], b_levels)).unsqueeze(0))\n",
    "            else:\n",
    "                multiset_list.append(torchhd.multiset(torchhd.bind(id, level)))\n",
    "        return torch.stack(multiset_list)\n",
    "\n",
    "    def forward(self, channels: torch.Tensor) -> torch.Tensor:\n",
    "        # Move input to CPU\n",
    "        channels = channels.to(self.cpu_device)\n",
    "\n",
    "        motion = channels[:, :, : self.input_size - 1]\n",
    "        hr = channels[:, :, self.input_size - 1].unsqueeze(-1)\n",
    "\n",
    "        motion_hv = self.motion_embed(motion)\n",
    "        hr_hv = self.hr_embed(hr)\n",
    "        combined = torch.cat([motion_hv, hr_hv], dim=2)\n",
    "\n",
    "        hvs = self.batch_generic(self.keys.weight, combined, self.ngrams)\n",
    "        sample = torchhd.multiset(hvs)\n",
    "        sample = torchhd.hard_quantize(sample)\n",
    "\n",
    "        return sample.to(self.device)  # Return tensor back to main device (e.g., MPS)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class HdcModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        out_dimension: int,\n",
    "        ngrams: int = 5,\n",
    "        dtype=torch.float32,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        super(HdcModel, self).__init__()\n",
    "        \n",
    "        self.encoder = HdcGenericEncoder(input_size, out_dimension, ngrams=ngrams, dtype=dtype, device=device)\n",
    "        self.centroid = models.Centroid(\n",
    "                out_dimension,\n",
    "                NUM_TAC_LEVELS,\n",
    "                dtype=dtype,\n",
    "                device=device,\n",
    "            )\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def add(self, x : torch.Tensor, y : torch.Tensor, lr : float):\n",
    "        hv = self.encoder(x)\n",
    "        labels = y.to(dtype=torch.int64)\n",
    "        for i in range(len(hv)):\n",
    "            #This operations can't be done in batches\n",
    "            self.centroid.add_adjust(\n",
    "                    hv[i].unsqueeze(0), labels[i], lr=lr\n",
    "                )\n",
    "            \n",
    "    def adjust_reset(self):\n",
    "        self.centroid.adjust_reset()\n",
    "        \n",
    "    #Executorch safe (0.6.x)\n",
    "    def vector_norm(self, x, p=2, dim=None, keepdim=False):\n",
    "        return torch.pow(torch.sum(torch.abs(x) ** p, dim=dim, keepdim=keepdim), 1 / p)\n",
    "        \n",
    "    def normalized_inference(self, input: torch.Tensor, dot: bool = False):\n",
    "        normalized_weight = self.centroid.weight.detach().clone()\n",
    "        norms = self.vector_norm(normalized_weight, p=2, dim=1, keepdim=True)\n",
    "        norms.clamp_(min=1e-12)\n",
    "        normalized_weight.div_(norms)\n",
    "\n",
    "        if dot:\n",
    "            return torchhd.functional.dot_similarity(input, normalized_weight)\n",
    "        return torchhd.functional.cosine_similarity(input, normalized_weight)\n",
    "        \n",
    "    def binary_hdc_output(self, outputs):\n",
    "        probs = F.softmax(outputs, dim=1)  # Shape: (batch_size, 2)\n",
    "        return probs[:, 1]  # Extract only class 1 probability\n",
    "        \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        hv = self.encoder(x)\n",
    "        output = self.normalized_inference(hv, True)\n",
    "\n",
    "        return self.binary_hdc_output(output)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def prepare_sequences_undersampled(df, window_size=10, overlap_ratio=0.1, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Generate sequences and apply undersampling to balance class distribution.\n",
    "    \"\"\"\n",
    "    session_user_map = pd.Series(df[\"user_id\"].values, index=df[\"session_id\"]).astype(int).to_dict()\n",
    "    if feature_columns is None:\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "    print(f\"Preparing sequences with undersampling...\")\n",
    "\n",
    "    features = df[feature_columns].values\n",
    "    labels = df[\"tac_flg\"].values\n",
    "    session_ids = df[\"session_id\"].values\n",
    "    unique_sessions = np.unique(session_ids)\n",
    "\n",
    "    sequences = []\n",
    "    sequence_labels = []\n",
    "    sequence_user_ids = []\n",
    "\n",
    "    for session_id in tqdm(unique_sessions):\n",
    "        session_mask = session_ids == session_id\n",
    "        session_indices = np.where(session_mask)[0]\n",
    "\n",
    "        if len(session_indices) >= window_size:\n",
    "            for start_idx in range(0, len(session_indices) - window_size + 1, int(window_size * overlap_ratio)):\n",
    "                window_indices = session_indices[start_idx : start_idx + window_size]\n",
    "                if len(window_indices) < window_size:\n",
    "                    continue\n",
    "\n",
    "                sequence = features[window_indices]\n",
    "                label = int(stats.mode(labels[window_indices].astype(int), keepdims=False).mode)\n",
    "\n",
    "                sequences.append(sequence)\n",
    "                sequence_labels.append(label)\n",
    "                sequence_user_ids.append(session_user_map[session_id])\n",
    "\n",
    "    sequences = np.array(sequences)\n",
    "    sequence_labels = np.array(sequence_labels)\n",
    "    sequence_user_ids = np.array(sequence_user_ids)\n",
    "\n",
    "    # === Undersample the majority class (label == 1)\n",
    "    print(\"Balancing sequences via undersampling...\")\n",
    "\n",
    "    sober_mask = sequence_labels == 1\n",
    "    drunk_mask = sequence_labels == 0\n",
    "\n",
    "    sober_seqs = sequences[sober_mask]\n",
    "    sober_labels = sequence_labels[sober_mask]\n",
    "    sober_users = sequence_user_ids[sober_mask]\n",
    "\n",
    "    drunk_seqs = sequences[drunk_mask]\n",
    "    def augment_drunk(seqs, labels, users, n_aug=1):\n",
    "        new_seqs, new_labels, new_users = [], [], []\n",
    "        for i in range(len(seqs)):\n",
    "            for _ in range(n_aug):\n",
    "                aug_seq = add_noise(seqs[i], level=0.01)\n",
    "                new_seqs.append(aug_seq)\n",
    "                new_labels.append(labels[i])\n",
    "                new_users.append(users[i])\n",
    "        return (\n",
    "            np.concatenate([seqs, np.stack(new_seqs)]),\n",
    "            np.concatenate([labels, new_labels]),\n",
    "            np.concatenate([users, new_users]),\n",
    "        )\n",
    "\n",
    "    \n",
    "    drunk_labels = sequence_labels[drunk_mask]\n",
    "    drunk_users = sequence_user_ids[drunk_mask]\n",
    "    \n",
    "    drunk_seqs, drunk_labels, drunk_users = augment_drunk(drunk_seqs, drunk_labels, drunk_users, n_aug=1)\n",
    "    if len(drunk_seqs) == 0 or len(sober_seqs) == 0:\n",
    "        raise ValueError(\"Insufficient class samples for undersampling.\")\n",
    "\n",
    "    undersample_ratio = 1  # or test 1.2, 1.3...\n",
    "    n_samples = min(len(sober_seqs), int(len(drunk_seqs) * undersample_ratio))\n",
    "\n",
    "\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "    if len(sober_seqs) > n_samples:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=(len(sober_seqs) - n_samples), random_state=42)\n",
    "        idxs_to_keep, _ = next(sss.split(sober_seqs, sober_users))\n",
    "        sober_seqs_resampled = sober_seqs[idxs_to_keep]\n",
    "        sober_labels_resampled = sober_labels[idxs_to_keep]\n",
    "        sober_users_resampled = sober_users[idxs_to_keep]\n",
    "    else:\n",
    "        # Not enough to subsample, just keep all\n",
    "        sober_seqs_resampled = sober_seqs\n",
    "        sober_labels_resampled = sober_labels\n",
    "        sober_users_resampled = sober_users\n",
    "\n",
    "\n",
    "\n",
    "    # Combine and shuffle\n",
    "    sequences_balanced = np.concatenate([sober_seqs_resampled, drunk_seqs])\n",
    "    labels_balanced = np.concatenate([sober_labels_resampled, drunk_labels])\n",
    "    users_balanced = np.concatenate([sober_users_resampled, drunk_users])\n",
    "\n",
    "    indices = np.arange(len(labels_balanced))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    sequences_balanced = sequences_balanced[indices]\n",
    "    labels_balanced = labels_balanced[indices]\n",
    "    users_balanced = users_balanced[indices]\n",
    "\n",
    "    print(f\"Undersampled sequence shape: {sequences_balanced.shape}\")\n",
    "\n",
    "    return sequences_balanced, labels_balanced, users_balanced\n",
    "\n",
    "def prepare_sequences_fast(df, window_size=10, overlap_ratio=0.1, feature_columns=None):\n",
    "        session_user_map = pd.Series(df[\"user_id\"].values, index=df[\"session_id\"]).astype(int).to_dict()\n",
    "        if feature_columns is None:\n",
    "            feature_columns = [\n",
    "                \"ZVALUEX_acc\",\n",
    "                \"ZVALUEY_acc\",\n",
    "                \"ZVALUEZ_acc\",\n",
    "                \"ZVALUEX_gyro\",\n",
    "                \"ZVALUEY_gyro\",\n",
    "                \"ZVALUEZ_gyro\",\n",
    "                \"ZHEARTRATE\",\n",
    "            ]\n",
    "\n",
    "        print(f\"Starting sequence preparation...\")\n",
    "\n",
    "        # 特徴量とラベルを事前にNumPy配列に変換\n",
    "        features = df[feature_columns].values\n",
    "        labels = df[\"tac_flg\"].values\n",
    "        session_ids = df[\"session_id\"].values\n",
    "\n",
    "        # セッションのユニークなIDと各セッションのインデックスを取得\n",
    "        unique_sessions = np.unique(session_ids)\n",
    "        print(f\"Processing {len(unique_sessions)} sessions...\")\n",
    "\n",
    "        sequences = []\n",
    "        sequence_labels = []\n",
    "        sequence_user_ids = []\n",
    "\n",
    "        for session_id in tqdm(unique_sessions):\n",
    "            session_mask = session_ids == session_id\n",
    "            session_indices = np.where(session_mask)[0]\n",
    "\n",
    "            if len(session_indices) >= window_size:\n",
    "                for start_idx in range(\n",
    "                    0, len(session_indices) - window_size + 1, np.ceil(window_size*overlap_ratio).astype(int)\n",
    "                ):\n",
    "                    window_indices = session_indices[start_idx : start_idx + window_size]\n",
    "                    if len(window_indices) < window_size:\n",
    "                        continue\n",
    "\n",
    "                    sequence = features[window_indices]\n",
    "                    # get the mode label in the current window\n",
    "                    label = stats.mode(labels[window_indices].astype(int))[0]\n",
    "\n",
    "                    sequences.append(sequence)\n",
    "                    sequence_labels.append(label)\n",
    "                    sequence_user_ids.append(session_user_map[session_id])\n",
    "\n",
    "        sequences = np.array(sequences)\n",
    "        sequence_labels = np.array(sequence_labels)\n",
    "\n",
    "        print(f\"Created {len(sequences)} sequences\")\n",
    "        print(f\"Sequences shape: {sequences.shape}\")\n",
    "\n",
    "        return sequences, sequence_labels, sequence_user_ids\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, criterion, device):\n",
    "    \"\"\"検証データでのモデル評価\"\"\"\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in valid_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_preds.extend(outputs.cpu().numpy())\n",
    "            val_labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    val_preds = np.array(val_preds)\n",
    "    val_labels = np.array(val_labels)\n",
    "\n",
    "    val_prauc = average_precision_score(val_labels, val_preds)\n",
    "    val_rocauc = roc_auc_score(val_labels, val_preds)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss / len(valid_loader),\n",
    "        \"pr_auc\": val_prauc,\n",
    "        \"roc_auc\": val_rocauc,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model : HdcModel,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    criterion,\n",
    "    lr,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    patience=5,\n",
    "):\n",
    "    best_val_prauc = 0\n",
    "    patience_counter = 0\n",
    "    best_train_epoch = 0\n",
    "    best_model_state = None\n",
    "    training_history = []\n",
    "\n",
    "    print(\n",
    "        \"Epoch | Train Loss |  Val Loss  |  Val PR-AUC  |  Val ROC-AUC  |  Epoch Time (s)\"\n",
    "    )\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            model.add(batch_X, batch_y, lr)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.squeeze(-1))\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        model.adjust_reset()\n",
    "\n",
    "        if valid_loader is None:\n",
    "            epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"time\": epoch_time}\n",
    "            training_history.append(epoch_results)\n",
    "            print(f\"{epoch+1:5d} | {train_loss:.6f} | ------ | ------ | {epoch_time:.2f}\")\n",
    "            continue\n",
    "    \n",
    "        val_metrics = validate_model(model, valid_loader, criterion, device)\n",
    "        epoch_results = {\"epoch\": epoch + 1, \"train_loss\": train_loss, \"val_loss\": val_metrics[\"loss\"], \n",
    "                         \"val_pr_auc\": val_metrics[\"pr_auc\"], \"val_roc_auc\": val_metrics[\"roc_auc\"], \"time\": epoch_time}\n",
    "        training_history.append(epoch_results)\n",
    "\n",
    "        print(\n",
    "            f\"{epoch+1:5d} | {train_loss:.6f} | {val_metrics['loss']:.6f} | \"\n",
    "            f\"{val_metrics['pr_auc']:.4f} | {val_metrics['roc_auc']:.4f} | \"\n",
    "            f\"{epoch_time:.2f}\"\n",
    "        )\n",
    "        \n",
    "        if val_metrics[\"pr_auc\"] >= best_val_prauc:\n",
    "            best_val_prauc = val_metrics[\"pr_auc\"]\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "            best_train_epoch = epoch\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after epoch {epoch+1}\")\n",
    "            print(f\"Best validation PR-AUC: {best_val_prauc:.4f}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None: \n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model, training_history, best_train_epoch\n",
    "\n",
    "\n",
    "def inference_dataset(model, data_loader, device, pred_threshold=None):\n",
    "    \"\"\"evaluation model after a fold training\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            labels.extend(batch_y.squeeze(-1).cpu().numpy())\n",
    "\n",
    "    pred_prob = np.array(predictions)\n",
    "    gt_labels = np.array(labels)\n",
    "    return pred_prob, gt_labels\n",
    "\n",
    "\n",
    "def performance_calculation(pred_prob, gt_label, threshold=None):\n",
    "    '''\n",
    "    Calculate the performance of the model\n",
    "    Args:\n",
    "    pred_prob: list, predicted probability\n",
    "    gt_label: list, ground truth label\n",
    "    threshold: float, threshold for binary classification (None if we are evaluating on train data)\n",
    "    '''\n",
    "    if threshold is None:\n",
    "        # Find the optimal threshold by maximizing F1 score\n",
    "        thresholds = np.linspace(0.01, 0.99, 99)  # Test 99 threshold values\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5  # Default if no better threshold is found\n",
    "        \n",
    "        for t in thresholds:\n",
    "            temp_pred = (pred_prob >= t).astype(int)\n",
    "            temp_f1 = (f1_score(gt_label, temp_pred, pos_label=0) + f1_score(gt_label, temp_pred, pos_label=1)) / 2\n",
    "\n",
    "            \n",
    "            if temp_f1 > best_f1:\n",
    "                best_f1 = temp_f1\n",
    "                best_threshold = t\n",
    "        \n",
    "        threshold = best_threshold\n",
    "        \n",
    "    pred_label = (pred_prob >= threshold).astype(int)\n",
    "    roc_auc = roc_auc_score(gt_label, pred_prob)\n",
    "    pr_auc = average_precision_score(gt_label, pred_prob)\n",
    "    accuracy = accuracy_score(gt_label, pred_label)\n",
    "\n",
    "    # Since 0 = drunk and 1 = sober\n",
    "    drunk_acc = accuracy_score(gt_label[gt_label == 0], pred_label[gt_label == 0])\n",
    "    sober_acc = accuracy_score(gt_label[gt_label == 1], pred_label[gt_label == 1])\n",
    "\n",
    "    f1 = f1_score(gt_label, pred_label)\n",
    "    return roc_auc, pr_auc, accuracy, sober_acc, drunk_acc, f1, threshold\n",
    "\n",
    "\n",
    "def generate_configs(base_config):\n",
    "    \"\"\"\n",
    "    Generate multiple configurations from a base config.\n",
    "    For any list values in the base config, create a separate config for each list item.\n",
    "    \n",
    "    Args:\n",
    "        base_config (dict): Base configuration with potential list values\n",
    "        \n",
    "    Returns:\n",
    "        list: List of individual configurations\n",
    "    \"\"\"\n",
    "    # Find all keys with list values\n",
    "    list_keys = [key for key, value in base_config.items() if isinstance(value, list)]\n",
    "    \n",
    "    if not list_keys:\n",
    "        # If no list values found, return the original config\n",
    "        return [base_config]\n",
    "    \n",
    "    # Start with the first list key\n",
    "    key = list_keys[0]\n",
    "    values = base_config[key]\n",
    "    \n",
    "    # Generate configurations for each value of the first list key\n",
    "    configs = []\n",
    "    for value in values:\n",
    "        # Create a new config with this specific value\n",
    "        new_config = base_config.copy()\n",
    "        new_config[key] = value\n",
    "        \n",
    "        # Recursively handle any remaining list keys\n",
    "        remaining_configs = generate_configs(new_config)\n",
    "        configs.extend(remaining_configs)\n",
    "    \n",
    "    return configs\n",
    "\n",
    "def train_rf_classifier():\n",
    "    df = load_data()\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "    # Normalize data\n",
    "    feature_cols = [\"ZVALUEX_acc\", \"ZVALUEY_acc\", \"ZVALUEZ_acc\", \"ZVALUEX_gyro\", \"ZVALUEY_gyro\", \"ZVALUEZ_gyro\"]\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    # Prepare sequences\n",
    "    window_size = 800\n",
    "    overlap_ratio = 0.5\n",
    "    X_train, y_train, _ = prepare_sequences_undersampled(df[df[\"user_id\"].isin(TRAIN_USERS[0] + VALID_USERS[0])], window_size, overlap_ratio)\n",
    "    X_test, y_test, _ = prepare_sequences_fast(df[df[\"user_id\"].isin(TEST_USERS)], window_size, overlap_ratio)\n",
    "\n",
    "    print(\"Encoding with HDC encoder...\")\n",
    "    encoder = HdcGenericEncoder(input_size=X_train.shape[2], out_dimension=5000, ngrams=7, device=device)\n",
    "    encoder.to(device)\n",
    "\n",
    "    X_train_hdc = extract_hdc_embeddings(encoder, X_train, device=device)\n",
    "    X_test_hdc = extract_hdc_embeddings(encoder, X_test, device=device)\n",
    "\n",
    "    print(f\"HDC Train shape: {X_train_hdc.shape}, Test shape: {X_test_hdc.shape}\")\n",
    "\n",
    "    print(\"Training Random Forest...\")\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(X_train_hdc, y_train)\n",
    "\n",
    "    preds = clf.predict(X_test_hdc)\n",
    "    proba = clf.predict_proba(X_test_hdc)[:, 1]\n",
    "\n",
    "    print(\"Evaluation:\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, preds))\n",
    "    print(f\"ROC AUC: {roc_auc_score(y_test, proba):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, preds):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, preds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_and_results(model, save_folder, train_preds, train_gt_labels, test_preds, test_gt_labels, metrics, config):\n",
    "    \"\"\"\n",
    "    Save model, predictions, ground truth, metrics, and model structure to the specified folder.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        save_folder: Folder path to save results\n",
    "        train_preds: Training predictions\n",
    "        train_gt_labels: Training ground truth labels\n",
    "        test_preds: Test predictions\n",
    "        test_gt_labels: Test ground truth labels\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "        config: Model configuration dictionary\n",
    "    \"\"\"\n",
    "    # Create a timestamp for the save files\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = os.path.join(save_folder, f\"model_{timestamp}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Save model architecture as text\n",
    "    model_structure_path = os.path.join(save_folder, f\"model_structure_{timestamp}.txt\")\n",
    "    with open(model_structure_path, 'w') as f:\n",
    "        f.write(str(model))\n",
    "    \n",
    "    # Save predictions and ground truth\n",
    "    predictions_data = {\n",
    "        'train_predictions': train_preds.tolist() if isinstance(train_preds, np.ndarray) else train_preds,\n",
    "        'train_ground_truth': train_gt_labels.tolist() if isinstance(train_gt_labels, np.ndarray) else train_gt_labels,\n",
    "        'test_predictions': test_preds.tolist() if isinstance(test_preds, np.ndarray) else test_preds,\n",
    "        'test_ground_truth': test_gt_labels.tolist() if isinstance(test_gt_labels, np.ndarray) else test_gt_labels\n",
    "    }\n",
    "    pred_path = os.path.join(save_folder, f\"predictions_{timestamp}.pkl\")\n",
    "    with open(pred_path, 'wb') as f:\n",
    "        pickle.dump(predictions_data, f)\n",
    "    \n",
    "    # Save all metrics\n",
    "    metrics_path = os.path.join(save_folder, f\"metrics_{timestamp}.json\")\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        json.dump(metrics, f, indent=4)\n",
    "    \n",
    "    # Save the configuration\n",
    "    config_path = os.path.join(save_folder, f\"config_{timestamp}.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(f\"Model, predictions, ground truth, and metrics saved in {save_folder}\")\n",
    "\n",
    "\n",
    "def train_and_eval_final_model(best_config, best_threshold, df):\n",
    "    print('\\nBEGIN TRAIN AND EVALUATION FINAL MODEL\\n')\n",
    "    feature_columns = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        \"ZHEARTRATE\",\n",
    "    ]\n",
    "\n",
    "    # Hyper parameter loading\n",
    "    device = best_config['device'] \n",
    "    window_size = best_config['window_size']\n",
    "    input_size = len(feature_columns)\n",
    "    batch_size = best_config['batch_size']\n",
    "    hdc_dimension = best_config['hdc_dimension']\n",
    "    ngrams = best_config['ngrams']\n",
    "    learning_rate = best_config['learning_rate']\n",
    "    epochs = best_config['epochs']\n",
    "    patience = best_config['patience']\n",
    "    runtime_log_fld = best_config['runtime_log_fld']\n",
    "    overlap_ratio = best_config['overlap_ratio']\n",
    "    \n",
    "    train_user = list(set(ALL_USERS)- set(TEST_USERS))\n",
    "    test_user = TEST_USERS\n",
    "        \n",
    "    train_data = df[df['user_id'].isin(train_user)]\n",
    "    test_data = df[df['user_id'].isin(test_user)]\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    scaler = StandardScaler()\n",
    "    train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "    test_data[columns_to_standardize] = scaler.transform(test_data[columns_to_standardize])\n",
    "\n",
    "    print(\"Preparing sequences...\")\n",
    "    X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "    X_test, y_test, test_user_ids = prepare_sequences_fast(test_data, window_size, overlap_ratio)\n",
    "\n",
    "\n",
    "    print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "    print(f\"Users in test:{set(test_data['user_id'])}\")\n",
    "    print(f\"Number of windows for training:{len(X_train)}\")\n",
    "    print(f\"Number of windows for testing:{len(X_test)}\")\n",
    "\n",
    "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "    test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 02. Train model (set patience to ensure that the model is trained for the best epoch)\n",
    "    model, training_history, _ = train_model(\n",
    "        model=model, train_loader=train_loader, valid_loader=None, \n",
    "        criterion=criterion, lr=learning_rate, device=device, epochs=epochs\n",
    "    )\n",
    "\n",
    "    # 03. Inference\n",
    "    train_preds, train_gt_labels = inference_dataset(model, train_loader, device)\n",
    "    test_preds, test_gt_labels = inference_dataset(model, test_loader, device)\n",
    "\n",
    "    # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "    train_roc_auc, train_pr_auc, train_accuracy, train_sober_acc, train_drunk_acc, train_f1, train_threshold = performance_calculation(train_preds, train_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Training ROC-AUC: {train_roc_auc:.4f}, PR-AUC: {train_pr_auc:.4f}, Accuracy: {train_accuracy:.4f}, Sober Accuracy: {train_sober_acc:.4f}, Drunk Accuracy: {train_drunk_acc:.4f}, F1: {train_f1:.4f}, Threshold: {train_threshold:.4f}\")\n",
    "    test_roc_auc, test_pr_auc, test_accuracy, test_sober_acc, test_drunk_acc, test_f1, test_threshold = performance_calculation(test_preds, test_gt_labels, threshold=best_threshold)\n",
    "    print(f\"Test ROC-AUC: {test_roc_auc:.4f}, PR-AUC: {test_pr_auc:.4f}, Accuracy: {test_accuracy:.4f}, Sober Accuracy: {test_sober_acc:.4f}, Drunk Accuracy: {test_drunk_acc:.4f}, F1: {test_f1:.4f}, Threshold: {test_threshold:.4f}\")    \n",
    "\n",
    "    # 05. Save model, predictions, ground truth, metrics and model structure\n",
    "    metrics = {\n",
    "        'train': {\n",
    "            'roc_auc': train_roc_auc,\n",
    "            'pr_auc': train_pr_auc,\n",
    "            'accuracy': train_accuracy,\n",
    "            'sober_accuracy': train_sober_acc,\n",
    "            'drunk_accuracy': train_drunk_acc,\n",
    "            'f1': train_f1,\n",
    "            'threshold': train_threshold\n",
    "        },\n",
    "        'test': {\n",
    "            'roc_auc': test_roc_auc,\n",
    "            'pr_auc': test_pr_auc,\n",
    "            'accuracy': test_accuracy,\n",
    "            'sober_accuracy': test_sober_acc,\n",
    "            'drunk_accuracy': test_drunk_acc,\n",
    "            'f1': test_f1,\n",
    "            'threshold': test_threshold\n",
    "        },\n",
    "        'config': best_config\n",
    "    }\n",
    "    \n",
    "    # Call the function to save all results\n",
    "    save_model_and_results(\n",
    "        model=model,\n",
    "        save_folder=runtime_log_fld,\n",
    "        train_preds=train_preds,\n",
    "        train_gt_labels=train_gt_labels,\n",
    "        test_preds=test_preds,\n",
    "        test_gt_labels=test_gt_labels,\n",
    "        metrics=metrics,\n",
    "        config=best_config\n",
    "    )\n",
    "    \n",
    "    # refresh GPU\n",
    "    model.to(\"cpu\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return train_accuracy, test_accuracy, metrics\n",
    "\n",
    "def train_cross_validation(df, all_configs):\n",
    "    print(\"=\"*50 + \"\\nBEGIN CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    best_config = None \n",
    "    best_pr_auc = 0\n",
    "    best_threshold = 0.5 \n",
    "    for config_idx, config in enumerate(all_configs):\n",
    "        print(f\"\\nCONFIG {config_idx}: {config}\\n\")\n",
    "        feature_columns = [\n",
    "            \"ZVALUEX_acc\",\n",
    "            \"ZVALUEY_acc\",\n",
    "            \"ZVALUEZ_acc\",\n",
    "            \"ZVALUEX_gyro\",\n",
    "            \"ZVALUEY_gyro\",\n",
    "            \"ZVALUEZ_gyro\",\n",
    "            \"ZHEARTRATE\",\n",
    "        ]\n",
    "\n",
    "        # Hyper parameter loading\n",
    "        device = config['device'] \n",
    "        window_size = config['window_size']\n",
    "        input_size = len(feature_columns)\n",
    "        batch_size = config['batch_size']\n",
    "        hdc_dimension = config['hdc_dimension']\n",
    "        ngrams = config['ngrams']\n",
    "        learning_rate = config['learning_rate']\n",
    "        epochs = config['epochs']\n",
    "        overlap_ratio = config['overlap_ratio']\n",
    "        patience = config['patience']\n",
    "\n",
    "        # variables for temperary storing\n",
    "        val_all_preds = []\n",
    "        val_all_gt_labels = []\n",
    "        val_trained_epoch = []\n",
    "\n",
    "        for fold, (train_user, val_user) in enumerate(zip(TRAIN_USERS, VALID_USERS)):\n",
    "\n",
    "            # 01. Prepare data and define model\n",
    "            print(\"-\" * 100)\n",
    "            print('FOLD:', fold+1)\n",
    "            print('TRAIN:', train_user)\n",
    "            print('VAL:', val_user)\n",
    "            \n",
    "            train_data = df[df['user_id'].isin(train_user)].copy()\n",
    "            val_data = df[df['user_id'].isin(val_user)].copy()\n",
    "\n",
    "            # columns will be normalized\n",
    "            columns_to_standardize = [\n",
    "                \"ZVALUEX_acc\",\n",
    "                \"ZVALUEY_acc\",\n",
    "                \"ZVALUEZ_acc\",\n",
    "                \"ZVALUEX_gyro\",\n",
    "                \"ZVALUEY_gyro\",\n",
    "                \"ZVALUEZ_gyro\",\n",
    "                #'ZHEARTRATE'\n",
    "            ]\n",
    "\n",
    "            # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "            scaler = StandardScaler()\n",
    "            train_data[columns_to_standardize] = scaler.fit_transform(train_data[columns_to_standardize])\n",
    "            val_data[columns_to_standardize] = scaler.transform(val_data[columns_to_standardize])\n",
    "\n",
    "            print(\"Preparing sequences...\")\n",
    "            X_train, y_train, train_user_ids = prepare_sequences_undersampled(train_data, window_size, overlap_ratio)\n",
    "            X_val, y_val, val_user_ids = prepare_sequences_undersampled(val_data, window_size, overlap_ratio)\n",
    "\n",
    "            print(f\"Users in train:{set(train_data['user_id'])}\")\n",
    "            print(f\"Users in test:{set(val_data['user_id'])}\")\n",
    "            print(f\"Number of windows for training:{len(X_train)}\")\n",
    "            print(f\"Number of windows for testing:{len(X_val)}\")\n",
    "\n",
    "            train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "            val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "            model = HdcModel(input_size, hdc_dimension, ngrams, device=device)\n",
    "\n",
    "            criterion = nn.BCELoss()\n",
    "            #optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # 02. Train model \n",
    "            model, training_history, train_best_epoch = train_model(\n",
    "                model, train_loader, val_loader, criterion, lr=learning_rate, device=device, \n",
    "                epochs=epochs, patience=patience\n",
    "            )\n",
    "\n",
    "            # 03. Inference\n",
    "            val_preds, val_gt_labels = inference_dataset(model, val_loader, device)\n",
    "            val_all_preds.append(val_preds)\n",
    "            val_all_gt_labels.append(val_gt_labels)\n",
    "            val_trained_epoch.append(train_best_epoch)\n",
    "\n",
    "            # refresh GPU\n",
    "            model.to(\"cpu\")\n",
    "            del model\n",
    "            gc.collect()\n",
    "\n",
    "        # 04. Calculate performance of current config: ROC, PR-AUC, ACC, F1, Drunk ACC, Sober ACC\n",
    "        val_all_preds = np.concatenate(val_all_preds)\n",
    "        val_all_gt_labels = np.concatenate(val_all_gt_labels)\n",
    "        val_roc_auc, val_pr_auc, val_accuracy, val_sober_acc, val_drunk_acc, val_f1, val_threshold = performance_calculation(val_preds, val_gt_labels)\n",
    "        print(f\"Validation ROC-AUC: {val_roc_auc:.4f}, PR-AUC: {val_pr_auc:.4f}, Accuracy: {val_accuracy:.4f}, Sober Accuracy: {val_sober_acc:.4f}, Drunk Accuracy: {val_drunk_acc:.4f}, F1: {val_f1:.4f}, Threshold: {val_threshold:.4f}\")\n",
    "        \n",
    "        # 05. set up best config\n",
    "        if val_pr_auc > best_pr_auc:\n",
    "            best_pr_auc = val_pr_auc\n",
    "            best_config = config\n",
    "            best_threshold = val_threshold\n",
    "            best_config_epoch = np.ceil(np.mean(val_trained_epoch)) + 1\n",
    "            print(f\"Updated best config: {best_config}, PR-AUC: {best_pr_auc:.4f}, Threshold: {best_threshold:.4f}, Epoch: {best_config_epoch:.2f}\")\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    best_config['epochs'] = int(best_config_epoch)\n",
    "    best_config['patience'] = int(best_config_epoch)\n",
    "    print(f'Final best config: {best_config}, Final best pr_auc: {best_pr_auc}, Final best threshold: {best_threshold}, Final best epoch: {best_config_epoch}')\n",
    "    print(\"=\"*50 + \"\\nEND CROSSVALIDATION\\n\" + \"=\"*50)    \n",
    "    return best_config, best_pr_auc, best_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file_path):\n",
    "    # Configure logging\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Remove existing handlers\n",
    "    for handler in logger.handlers[:]:\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Add file handler\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Create a custom print function\n",
    "    original_print = print\n",
    "    \n",
    "    def custom_print(*args, **kwargs):\n",
    "        # Call original print\n",
    "        # original_print(*args, **kwargs)\n",
    "        # Log the printed content\n",
    "        message = \" \".join(str(arg) for arg in args)\n",
    "        logger.info(f\"PRINT: {message}\")\n",
    "    \n",
    "    # Replace built-in print\n",
    "    import builtins\n",
    "    builtins.print = custom_print\n",
    "    \n",
    "    logger.info(f\"Logging initialized to {os.path.abspath(log_file_path)}\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def train_hdc_with_classifier(\n",
    "    classifier: BaseEstimator,\n",
    "    classifier_name: str,\n",
    "    df: pd.DataFrame,\n",
    "    config: dict,\n",
    "):\n",
    "    # 1. Create log folder and set up logging\n",
    "    runtime_log_fld = f\"results/{MODEL_NAME}/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    os.makedirs(runtime_log_fld, exist_ok=True)\n",
    "    setup_logging(f\"{runtime_log_fld}/training.log\")\n",
    "    \n",
    "    # 2. Normalize data\n",
    "    standard_cols = [\n",
    "        \"ZVALUEX_acc\", \"ZVALUEY_acc\", \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\", \"ZVALUEY_gyro\", \"ZVALUEZ_gyro\"\n",
    "    ]\n",
    "    scaler = StandardScaler()\n",
    "    df[standard_cols] = scaler.fit_transform(df[standard_cols])\n",
    "\n",
    "    # 3. Prepare sequences\n",
    "    X_train, y_train, _ = prepare_sequences_undersampled(\n",
    "        df[df[\"user_id\"].isin(TRAIN_USERS[0] + VALID_USERS[0])],\n",
    "        config[\"window_size\"], config[\"overlap_ratio\"]\n",
    "    )\n",
    "    X_test, y_test, _ = prepare_sequences_fast(\n",
    "        df[df[\"user_id\"].isin(TEST_USERS)],\n",
    "        config[\"window_size\"], config[\"overlap_ratio\"]\n",
    "    )\n",
    "\n",
    "    # 4. Encode with HDC\n",
    "    encoder = HdcGenericEncoder(\n",
    "        input_size=X_train.shape[2],\n",
    "        out_dimension=config[\"hdc_dimension\"],\n",
    "        ngrams=config[\"ngrams\"],\n",
    "        device=config[\"device\"]  # this is where encoded output will be moved\n",
    "    )   \n",
    "\n",
    "    X_train_hdc = extract_hdc_embeddings(encoder, X_train, device=config[\"device\"])\n",
    "    X_test_hdc = extract_hdc_embeddings(encoder, X_test, device=config[\"device\"])\n",
    "\n",
    "    print(f\"[{classifier_name}] Training classifier...\")\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=classifier,\n",
    "        param_grid=param_grid,\n",
    "        scoring='f1',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train_hdc, y_train)\n",
    "    classifier = grid_search.best_estimator_\n",
    "    print(\"Best MLP Parameters:\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "    # 5. Evaluation\n",
    "    preds = classifier.predict(X_test_hdc)\n",
    "    proba = classifier.predict_proba(X_test_hdc)[:, 1]\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, proba)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    print(classification_report(y_test, preds))\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # 6. Save everything\n",
    "    import joblib\n",
    "    joblib.dump(classifier, os.path.join(runtime_log_fld, f\"{classifier_name}_model.pkl\"))\n",
    "    torch.save(encoder.state_dict(), os.path.join(runtime_log_fld, \"encoder.pt\"))\n",
    "\n",
    "    with open(os.path.join(runtime_log_fld, \"predictions.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"test_predictions\": proba, \"test_ground_truth\": y_test.tolist()}, f)\n",
    "\n",
    "    with open(os.path.join(runtime_log_fld, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1_score\": f1,\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"best_params\": grid_search.best_params_  # new!\n",
    "        }, f, indent=2)\n",
    "\n",
    "\n",
    "    print(f\"[{classifier_name}] Results saved to {runtime_log_fld}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hdc_embeddings(encoder, sequences, batch_size=128, device=\"cpu\"):\n",
    "    encoder.eval()\n",
    "    all_hvs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch = torch.tensor(sequences[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            hvs = encoder(batch)\n",
    "            all_hvs.append(hvs.cpu().numpy())\n",
    "\n",
    "            # Add logging every 1000 samples\n",
    "            if i % 1 == 0 or i + batch_size >= len(sequences):\n",
    "                print(f\"Encoded {i+batch_size}/{len(sequences)} sequences\")\n",
    "\n",
    "    return np.concatenate(all_hvs, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runtime():\n",
    "    # 1.set up logging\n",
    "    runtime_log_fld = f\"results/{MODEL_NAME}/{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    if os.path.exists(runtime_log_fld) == False:\n",
    "        os.makedirs(runtime_log_fld)\n",
    "    logger = setup_logging(f\"{runtime_log_fld}/training.log\")\n",
    "    \n",
    "    # 2.set up configurations\n",
    "    base_configs = BASE_CONFIGS\n",
    "    base_configs['runtime_log_fld'] = runtime_log_fld   \n",
    "    all_configs = generate_configs(base_configs)\n",
    "    print(f\"Total configurations: {len(all_configs)}\")\n",
    "\n",
    "    # 3.Load the raw data\n",
    "    df = load_data()\n",
    "    # columns will be normalized\n",
    "    columns_to_standardize = [\n",
    "        \"ZVALUEX_acc\",\n",
    "        \"ZVALUEY_acc\",\n",
    "        \"ZVALUEZ_acc\",\n",
    "        \"ZVALUEX_gyro\",\n",
    "        \"ZVALUEY_gyro\",\n",
    "        \"ZVALUEZ_gyro\",\n",
    "        #'ZHEARTRATE'\n",
    "    ]\n",
    "\n",
    "    # # doing normalization (assume that we have the same scaler for all data, faster computing than do it separately)\n",
    "    # scaler = StandardScaler()\n",
    "    # df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "    # 4. Train and evaluate all configurations\n",
    "    # get the best config of current model \n",
    "    # get the best threshold of drunk or sober based on all fold validation data when using best config\n",
    "    best_config, best_pr_auc, best_threshold = train_cross_validation(df, all_configs)\n",
    "\n",
    "    # 5.Train and evaluate the final model with the best configuration\n",
    "    # train the final model on all data in cross validation and evaluate on test data\n",
    "    # calculate the performance and save the model, metrics and best config \n",
    "    train_and_eval_final_model(best_config, best_threshold, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:04<00:00, 10.31it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 21.49it/s]\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:37:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  58.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:38:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  51.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  53.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  52.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  55.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  50.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  56.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:39:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:40:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:15] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:41:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  38.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  38.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  39.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  36.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  35.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  36.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  36.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:42:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  35.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  38.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.8min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:43:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  43.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  42.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  42.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  43.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:21] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  40.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  40.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  39.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  40.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  45.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  55.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:44:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  47.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  49.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  46.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  43.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:56] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:45:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  45.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=  56.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=  59.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:46:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  37.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  38.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.5min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  36.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  38.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:14] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:18] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  39.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  40.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  42.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:47:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  59.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  58.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=  58.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:10] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:11] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  50.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.2min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  48.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  48.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  48.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:48:59] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:07] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  49.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  54.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  55.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  56.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=  58.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:49:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=  59.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=  59.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:46] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:50:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=  58.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=  59.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=  56.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:20] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:51:43] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  37.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  35.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  36.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  36.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.8min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:19] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 2.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.9min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=  37.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.8min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  36.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  36.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  36.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  37.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=  39.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:52:57] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.8min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:12] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  43.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  41.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  42.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=  59.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  41.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=  59.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:53:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:00] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:01] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  39.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:02] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:04] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  40.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:05] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  40.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  41.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:13] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  44.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:17] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=  48.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:54:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  47.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  47.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  47.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  45.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:16] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:22] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:27] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:36] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=  50.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=  59.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:55:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [11:56:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.4min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.3min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time= 1.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=  55.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=  54.1s\n"
     ]
    }
   ],
   "source": [
    "##runtime()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Define base config\n",
    "HDC_CONFIG = {\n",
    "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    \"window_size\": 800,\n",
    "    \"ngrams\": 7,\n",
    "    \"hdc_dimension\": 5000,\n",
    "    \"overlap_ratio\": 0.5\n",
    "}\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "\n",
    "train_hdc_with_classifier(\n",
    "    classifier=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    classifier_name=\"XGBoost\",\n",
    "    df=df,\n",
    "    config=HDC_CONFIG\n",
    ")\n",
    "# Try Random Forest\n",
    "# train_hdc_with_classifier(\n",
    "#     classifier=RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "#     classifier_name=\"RandomForest\",\n",
    "#     df=df,\n",
    "#     config=HDC_CONFIG\n",
    "# )\n",
    "\n",
    "# Try MLP\n",
    "# train_hdc_with_classifier(\n",
    "#     classifier=MLPClassifier(hidden_layer_sizes=(128,), max_iter=200, random_state=42),\n",
    "#     classifier_name=\"MLP\",\n",
    "#     df=df,\n",
    "#     config=HDC_CONFIG\n",
    "# )\n",
    "\n",
    "# Try SVM\n",
    "# train_hdc_with_classifier(\n",
    "#     classifier=SVC(probability=True, kernel='rbf', C=1.0, random_state=42),\n",
    "#     classifier_name=\"SVM\",\n",
    "#     df=df,\n",
    "#     config=HDC_CONFIG\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from glob import glob\n",
    "files = glob(\"../results/HDC/20250326_112132/predictions_20250326_120022.pkl\")\n",
    "assert len(files) > 0, \"No prediction files found!\"\n",
    "file_path = files[0]\n",
    "with open(file_path, \"rb\") as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "\n",
    "# Get raw test predictions and labels\n",
    "test_probs = np.array(predictions[\"test_predictions\"])\n",
    "test_labels = np.array(predictions[\"test_ground_truth\"])\n",
    "\n",
    "# Use same threshold as during training\n",
    "threshold = 0.31  # or dynamically extract it from your saved metrics\n",
    "\n",
    "# Convert to binary predictions\n",
    "test_binary_preds = (test_probs >= threshold).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_binary_preds)\n",
    "\n",
    "# Print it\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Predicted Drunk\", \"Predicted Sober\"],\n",
    "            yticklabels=[\"Actual Drunk\", \"Actual Sober\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11774797737598419"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['test_predictions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions['test_ground_truth'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count class labels\n",
    "from collections import Counter\n",
    "test_label_counts = Counter(predictions[\"test_ground_truth\"])\n",
    "train_label_counts = Counter(predictions[\"train_ground_truth\"])\n",
    "\n",
    "print(\"Class counts in TEST set:\")\n",
    "for label, count in sorted(test_label_counts.items()):\n",
    "    print(f\"Label {int(label)}: {count}\")\n",
    "\n",
    "print(\"\\nClass counts in TRAIN set:\")\n",
    "for label, count in sorted(train_label_counts.items()):\n",
    "    print(f\"Label {int(label)}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 33036, 1.0: 16576})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 2496, 1.0: 1462})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
